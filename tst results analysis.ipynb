{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54969651",
   "metadata": {},
   "source": [
    "# Time Series Transformer (TST) Sleep Analysis - Results & Clustering\n",
    "\n",
    "This notebook provides comprehensive analysis of sleep stage detection using Time Series Transformer (TST) embeddings combined with K-means clustering. This work represents advanced sequential learning approaches for automated sleep analysis as part of the research project conducted at Institut de Neurosciences des Syst√®mes (INS).\n",
    "\n",
    "## Research Methodology:\n",
    "- **Time Series Transformers**: Advanced deep learning architecture for capturing long-range temporal dependencies in physiological signals\n",
    "- **Multi-Scale Analysis**: Comparison between 3-second and 30-second time windows for different temporal resolution insights\n",
    "- **Unsupervised Clustering**: K-means clustering applied to transformer embeddings for sleep stage discovery\n",
    "- **Micro-arousal Detection**: High-resolution temporal analysis for detecting brief arousal events\n",
    "\n",
    "## Analysis Components:\n",
    "- **Cluster Distribution Analysis**: Statistical characterization of detected sleep patterns\n",
    "- **Temporal Dynamics**: Evolution of sleep stages across recording sessions\n",
    "- **Frequency Domain Analysis**: Spectral characteristics of different sleep clusters\n",
    "- **Comparative Analysis**: Performance evaluation across different time scales\n",
    "- **Clinical Interpretation**: Physiological relevance of detected patterns\n",
    "\n",
    "## Data Sources:\n",
    "- **3-second Analysis**: `predicted_labels_3s_tst_20_files_*.csv/npy` - High temporal resolution\n",
    "- **30-second Analysis**: `predicted_labels_30s_tst_3_files_*.csv/npy` - Standard sleep scoring resolution\n",
    "- **EEG Source**: `SC4001E0-PSG.edf` - Raw polysomnography data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# LIBRARY IMPORTS AND CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Signal processing and analysis\n",
    "import scipy.signal as signal\n",
    "from scipy import stats\n",
    "import mne\n",
    "\n",
    "# Machine learning and data analysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization styles for consistent appearance\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "px.defaults.template = 'plotly_white'\n",
    "\n",
    "print('‚úÖ All required libraries imported successfully!')\n",
    "print('‚úÖ Visualization styles configured for Time Series Transformer analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb31967",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Preprocessing\n",
    "\n",
    "Load Time Series Transformer clustering results and original EEG data for comprehensive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TIME SERIES TRANSFORMER RESULTS LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "def load_tst_clustering_results():\n",
    "    \"\"\"\n",
    "    Load Time Series Transformer clustering results for multi-scale analysis.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (results_3s, results_30s) dictionaries containing CSV, NPY, and metadata\n",
    "    \"\"\"\n",
    "    print(\"üìä LOADING TIME SERIES TRANSFORMER RESULTS\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Load 3-second window results (high temporal resolution)\n",
    "    print(\"üîç Loading 3-second window analysis results...\")\n",
    "    results_3s = {}\n",
    "    \n",
    "    try:\n",
    "        results_3s['csv'] = pd.read_csv('results/predicted_labels_3s_tst_20_files_20250530_131707.csv')\n",
    "        results_3s['npy'] = np.load('results/predicted_labels_3s_tst_20_files_20250530_131707.npy')\n",
    "        \n",
    "        with open('results/predicted_labels_3s_tst_20_files_20250530_131707_metadata.txt', 'r') as f:\n",
    "            results_3s['metadata'] = f.read()\n",
    "        \n",
    "        print(f\"  ‚úÖ 3s CSV shape: {results_3s['csv'].shape}\")\n",
    "        print(f\"  ‚úÖ 3s NPY shape: {results_3s['npy'].shape}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ‚ùå Error loading 3s results: {e}\")\n",
    "        results_3s = None\n",
    "    \n",
    "    # Load 30-second window results (standard sleep scoring resolution)\n",
    "    print(\"\\nüîç Loading 30-second window analysis results...\")\n",
    "    results_30s = {}\n",
    "    \n",
    "    try:\n",
    "        results_30s['csv'] = pd.read_csv('results/predicted_labels_30s_tst_3_files_20250523_181027.csv')\n",
    "        results_30s['npy'] = np.load('results/predicted_labels_30s_tst_3_files_20250523_181027.npy')\n",
    "        \n",
    "        with open('results/predicted_labels_30s_tst_3_files_20250523_181027_metadata.txt', 'r') as f:\n",
    "            results_30s['metadata'] = f.read()\n",
    "            \n",
    "        print(f\"  ‚úÖ 30s CSV shape: {results_30s['csv'].shape}\")\n",
    "        print(f\"  ‚úÖ 30s NPY shape: {results_30s['npy'].shape}\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ‚ùå Error loading 30s results: {e}\")\n",
    "        results_30s = None\n",
    "    \n",
    "    return results_3s, results_30s\n",
    "\n",
    "def load_source_eeg_data():\n",
    "    \"\"\"\n",
    "    Load original polysomnography (PSG) EEG data for signal analysis.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (eeg_signal, sampling_frequency, raw_mne_object)\n",
    "    \"\"\"\n",
    "    print(\"\\nüì° LOADING SOURCE EEG DATA\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    # Priority order for EEG file locations\n",
    "    eeg_file_paths = [\n",
    "        'by captain borat/raw/SC4001E0-PSG.edf',\n",
    "        'raw data/SC4001E0-PSG.edf',\n",
    "        'SC4001E0-PSG.edf'\n",
    "    ]\n",
    "    \n",
    "    for eeg_path in eeg_file_paths:\n",
    "        try:\n",
    "            print(f\"üîç Attempting to load: {eeg_path}\")\n",
    "            \n",
    "            # Load EDF file with MNE\n",
    "            raw = mne.io.read_raw_edf(eeg_path, preload=True, verbose=False)\n",
    "            \n",
    "            # Extract EEG signal (assuming first channel contains EEG)\n",
    "            eeg_signal = raw.get_data()[0]\n",
    "            sampling_freq = raw.info['sfreq']\n",
    "            \n",
    "            print(f\"  ‚úÖ EEG data loaded successfully\")\n",
    "            print(f\"  üìä Signal shape: {eeg_signal.shape}\")\n",
    "            print(f\"  üìä Sampling frequency: {sampling_freq} Hz\")\n",
    "            print(f\"  üìä Duration: {len(eeg_signal)/sampling_freq/3600:.2f} hours\")\n",
    "            print(f\"  üìä Channels available: {len(raw.ch_names)} ({', '.join(raw.ch_names[:5])}{'...' if len(raw.ch_names) > 5 else ''})\")\n",
    "            \n",
    "            return eeg_signal, sampling_freq, raw\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ‚ö†Ô∏è  File not found: {eeg_path}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error loading {eeg_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"  ‚ùå No EEG data files found - analysis will be limited to clustering results only\")\n",
    "    return None, None, None\n",
    "\n",
    "# Load all data sources\n",
    "results_3s, results_30s = load_tst_clustering_results()\n",
    "eeg_data, fs, raw_eeg = load_source_eeg_data()\n",
    "\n",
    "print(f\"\\nüìã DATA LOADING SUMMARY:\")\n",
    "print(f\"  3-second analysis: {'‚úÖ Available' if results_3s else '‚ùå Unavailable'}\")\n",
    "print(f\"  30-second analysis: {'‚úÖ Available' if results_30s else '‚ùå Unavailable'}\")\n",
    "print(f\"  Source EEG data: {'‚úÖ Available' if eeg_data is not None else '‚ùå Unavailable'}\")\n",
    "\n",
    "if results_3s and results_30s:\n",
    "    print(f\"\\nüî¨ MULTI-SCALE ANALYSIS READY:\")\n",
    "    print(f\"  High-resolution (3s): {len(results_3s['csv']):,} windows\")\n",
    "    print(f\"  Standard-resolution (30s): {len(results_30s['csv']):,} windows\")\n",
    "    \n",
    "print(\"‚úÖ Data loading completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00344711",
   "metadata": {},
   "source": [
    "# 2. Window Duration Validation and Cluster Distribution Analysis\n",
    "\n",
    "Validate the temporal windowing strategy and analyze the distribution of detected sleep clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b616eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# WINDOW DURATION VALIDATION\n",
    "# ==============================================================================\n",
    "\n",
    "def validate_window_durations(results_3s, results_30s):\n",
    "    \"\"\"\n",
    "    Validate temporal windowing parameters and analyze window characteristics.\n",
    "    \n",
    "    Args:\n",
    "        results_3s, results_30s: Loaded clustering results dictionaries\n",
    "    \"\"\"\n",
    "    print(\"‚è±Ô∏è  TEMPORAL WINDOWING VALIDATION\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Analyze 3-second windows\n",
    "    if results_3s:\n",
    "        print(\"üîç 3-Second Window Analysis:\")\n",
    "        df_3s = results_3s['csv']\n",
    "        \n",
    "        # Calculate actual window durations\n",
    "        durations_3s = df_3s['end_time_sec'] - df_3s['start_time_sec']\n",
    "        \n",
    "        print(f\"  Total windows: {len(df_3s):,}\")\n",
    "        print(f\"  Expected duration: 3.0s\")\n",
    "        print(f\"  Actual duration range: {durations_3s.min():.1f}s - {durations_3s.max():.1f}s\")\n",
    "        print(f\"  Mean duration: {durations_3s.mean():.2f}s (¬±{durations_3s.std():.3f}s)\")\n",
    "        \n",
    "        # Display first few windows for verification\n",
    "        print(f\"\\n  Sample windows:\")\n",
    "        for i in range(min(5, len(df_3s))):\n",
    "            start = df_3s['start_time_sec'].iloc[i]\n",
    "            end = df_3s['end_time_sec'].iloc[i] \n",
    "            duration = end - start\n",
    "            cluster = df_3s['predicted_cluster'].iloc[i]\n",
    "            print(f\"    Window {i:2d}: {start:6.1f}s - {end:6.1f}s (Œî{duration:.1f}s) ‚Üí Cluster {cluster}\")\n",
    "    \n",
    "    # Analyze 30-second windows\n",
    "    if results_30s:\n",
    "        print(f\"\\nüîç 30-Second Window Analysis:\")\n",
    "        df_30s = results_30s['csv']\n",
    "        \n",
    "        # Calculate actual window durations\n",
    "        durations_30s = df_30s['end_time_sec'] - df_30s['start_time_sec']\n",
    "        \n",
    "        print(f\"  Total windows: {len(df_30s):,}\")\n",
    "        print(f\"  Expected duration: 30.0s\")\n",
    "        print(f\"  Actual duration range: {durations_30s.min():.1f}s - {durations_30s.max():.1f}s\")\n",
    "        print(f\"  Mean duration: {durations_30s.mean():.2f}s (¬±{durations_30s.std():.3f}s)\")\n",
    "        \n",
    "        # Display first few windows for verification\n",
    "        print(f\"\\n  Sample windows:\")\n",
    "        for i in range(min(5, len(df_30s))):\n",
    "            start = df_30s['start_time_sec'].iloc[i]\n",
    "            end = df_30s['end_time_sec'].iloc[i]\n",
    "            duration = end - start\n",
    "            cluster = df_30s['predicted_cluster'].iloc[i]\n",
    "            print(f\"    Window {i:2d}: {start:6.1f}s - {end:6.1f}s (Œî{duration:.1f}s) ‚Üí Cluster {cluster}\")\n",
    "    \n",
    "    # Temporal coverage analysis\n",
    "    if results_3s and results_30s:\n",
    "        print(f\"\\nüìä Multi-Scale Coverage Comparison:\")\n",
    "        \n",
    "        total_time_3s = df_3s['end_time_sec'].max()\n",
    "        total_time_30s = df_30s['end_time_sec'].max()\n",
    "        \n",
    "        print(f\"  3s analysis coverage: {total_time_3s/3600:.2f} hours\")\n",
    "        print(f\"  30s analysis coverage: {total_time_30s/3600:.2f} hours\")\n",
    "        print(f\"  Temporal resolution ratio: {len(df_3s)/len(df_30s):.1f}:1\")\n",
    "        \n",
    "        # Check for temporal alignment\n",
    "        overlap_start = max(df_3s['start_time_sec'].min(), df_30s['start_time_sec'].min())\n",
    "        overlap_end = min(df_3s['end_time_sec'].max(), df_30s['end_time_sec'].max())\n",
    "        overlap_duration = max(0, overlap_end - overlap_start)\n",
    "        \n",
    "        print(f\"  Temporal overlap: {overlap_duration/3600:.2f} hours\")\n",
    "        print(f\"  Overlap percentage: {overlap_duration/max(total_time_3s, total_time_30s)*100:.1f}%\")\n",
    "\n",
    "# Perform window validation\n",
    "if results_3s or results_30s:\n",
    "    validate_window_durations(results_3s, results_30s)\n",
    "    print(\"‚úÖ Window duration validation completed\")\n",
    "else:\n",
    "    print(\"‚ùå No clustering results available for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CLUSTER DISTRIBUTION ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_tst_cluster_distribution(results, analysis_type):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of Time Series Transformer clustering results.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): TST clustering results containing CSV data\n",
    "        analysis_type (str): Description of analysis (e.g., \"3-second\", \"30-second\")\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (cluster_counts, cluster_percentages, analysis_summary)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results or 'csv' not in results:\n",
    "        print(f\"‚ùå No valid results available for {analysis_type} analysis\")\n",
    "        return None, None, None\n",
    "    \n",
    "    df = results['csv']\n",
    "    \n",
    "    print(f\"üìä TST CLUSTER DISTRIBUTION ANALYSIS ({analysis_type.upper()})\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Basic temporal parameters\n",
    "    window_duration = df['end_time_sec'].iloc[0] - df['start_time_sec'].iloc[0]\n",
    "    total_duration_hours = df['end_time_sec'].max() / 3600\n",
    "    \n",
    "    # Cluster distribution statistics\n",
    "    cluster_counts = df['predicted_cluster'].value_counts().sort_index()\n",
    "    cluster_percentages = (cluster_counts / len(df) * 100).round(2)\n",
    "    \n",
    "    print(f\"Recording Overview:\")\n",
    "    print(f\"  Total analysis windows: {len(df):,}\")\n",
    "    print(f\"  Window duration: {window_duration:.1f}s\")\n",
    "    print(f\"  Total recording time: {total_duration_hours:.2f}h ({total_duration_hours*60:.0f}min)\")\n",
    "    print(f\"  Temporal resolution: {3600/window_duration:.0f} windows/hour\")\n",
    "    \n",
    "    print(f\"\\nCluster Distribution:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for cluster in sorted(cluster_counts.index):\n",
    "        count = cluster_counts[cluster]\n",
    "        percentage = cluster_percentages[cluster]\n",
    "        duration_minutes = count * window_duration / 60\n",
    "        duration_hours = duration_minutes / 60\n",
    "        \n",
    "        print(f\"  Cluster {cluster}: {count:5,d} windows ({percentage:5.1f}%) \"\n",
    "              f\"‚Üí {duration_minutes:6.1f}min ({duration_hours:4.2f}h)\")\n",
    "    \n",
    "    # Identify dominant and rare clusters\n",
    "    dominant_cluster = cluster_counts.idxmax()\n",
    "    rare_cluster = cluster_counts.idxmin()\n",
    "    \n",
    "    print(f\"\\nCluster Characteristics:\")\n",
    "    print(f\"  Most frequent: Cluster {dominant_cluster} ({cluster_percentages[dominant_cluster]:.1f}%)\")\n",
    "    print(f\"  Least frequent: Cluster {rare_cluster} ({cluster_percentages[rare_cluster]:.1f}%)\")\n",
    "    print(f\"  Cluster diversity: {len(cluster_counts)} distinct sleep patterns detected\")\n",
    "    \n",
    "    return cluster_counts, cluster_percentages, {\n",
    "        'total_windows': len(df),\n",
    "        'total_duration_hours': total_duration_hours,\n",
    "        'window_duration': window_duration,\n",
    "        'dominant_cluster': dominant_cluster,\n",
    "        'dominant_percentage': cluster_percentages[dominant_cluster]\n",
    "    }\n",
    "\n",
    "def create_tst_distribution_plots(results_3s, results_30s):\n",
    "    \"\"\"Create comprehensive visualization comparing 3s and 30s clustering results.\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=[\n",
    "            '3s Windows: Cluster Counts', '30s Windows: Cluster Counts',\n",
    "            'Cluster Proportions Comparison',\n",
    "            '3s Timeline (First 2h)', '30s Timeline (First 2h)', \n",
    "            'Multi-Scale Summary'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'bar'}, {'type': 'bar'}],\n",
    "            [{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'table'}]\n",
    "        ],\n",
    "        horizontal_spacing=0.08,\n",
    "        vertical_spacing=0.12\n",
    "    )\n",
    "    \n",
    "    colors_3s = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "    colors_30s = ['#17becf', '#bcbd22', '#e377c2', '#8c564b', '#7f7f7f']\n",
    "    \n",
    "    # 3s cluster counts\n",
    "    if results_3s:\n",
    "        df_3s = results_3s['csv']\n",
    "        counts_3s = df_3s['predicted_cluster'].value_counts().sort_index()\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=counts_3s.index,\n",
    "            y=counts_3s.values,\n",
    "            name='3s Windows',\n",
    "            marker_color=colors_3s[0],\n",
    "            text=counts_3s.values,\n",
    "            textposition='outside'\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    # 30s cluster counts\n",
    "    if results_30s:\n",
    "        df_30s = results_30s['csv']\n",
    "        counts_30s = df_30s['predicted_cluster'].value_counts().sort_index()\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=counts_30s.index,\n",
    "            y=counts_30s.values,\n",
    "            name='30s Windows',\n",
    "            marker_color=colors_30s[0],\n",
    "            text=counts_30s.values,\n",
    "            textposition='outside'\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    # Comparative proportions\n",
    "    if results_3s and results_30s:\n",
    "        all_clusters = sorted(set(counts_3s.index) | set(counts_30s.index))\n",
    "        \n",
    "        props_3s = [(counts_3s.get(c, 0) / len(df_3s) * 100) for c in all_clusters]\n",
    "        props_30s = [(counts_30s.get(c, 0) / len(df_30s) * 100) for c in all_clusters]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=all_clusters,\n",
    "            y=props_3s,\n",
    "            name='3s (%)',\n",
    "            marker_color=colors_3s[0],\n",
    "            opacity=0.7\n",
    "        ), row=1, col=3)\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=all_clusters,\n",
    "            y=props_30s,\n",
    "            name='30s (%)',\n",
    "            marker_color=colors_30s[0],\n",
    "            opacity=0.7\n",
    "        ), row=1, col=3)\n",
    "    \n",
    "    # Timeline visualizations (first 2 hours)\n",
    "    if results_3s:\n",
    "        df_3s_subset = df_3s[df_3s['end_time_sec'] <= 7200].copy()\n",
    "        time_min_3s = df_3s_subset['start_time_sec'] / 60\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=time_min_3s,\n",
    "            y=df_3s_subset['predicted_cluster'],\n",
    "            mode='markers',\n",
    "            name='3s Timeline',\n",
    "            marker=dict(size=4, opacity=0.6),\n",
    "            showlegend=False\n",
    "        ), row=2, col=1)\n",
    "    \n",
    "    if results_30s:\n",
    "        df_30s_subset = df_30s[df_30s['end_time_sec'] <= 7200].copy()\n",
    "        time_min_30s = df_30s_subset['start_time_sec'] / 60\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=time_min_30s,\n",
    "            y=df_30s_subset['predicted_cluster'],\n",
    "            mode='markers',\n",
    "            name='30s Timeline',\n",
    "            marker=dict(size=8, opacity=0.8),\n",
    "            showlegend=False\n",
    "        ), row=2, col=2)\n",
    "    \n",
    "    # Summary table\n",
    "    summary_data = []\n",
    "    if results_3s:\n",
    "        summary_data.append(['3s Analysis', f'{len(df_3s):,}', f'{len(counts_3s)}', f'{df_3s[\"end_time_sec\"].max()/3600:.1f}h'])\n",
    "    if results_30s:\n",
    "        summary_data.append(['30s Analysis', f'{len(df_30s):,}', f'{len(counts_30s)}', f'{df_30s[\"end_time_sec\"].max()/3600:.1f}h'])\n",
    "    \n",
    "    if summary_data:\n",
    "        fig.add_trace(go.Table(\n",
    "            header=dict(values=['Analysis Type', 'Windows', 'Clusters', 'Duration']),\n",
    "            cells=dict(values=list(zip(*summary_data)))\n",
    "        ), row=2, col=3)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title={\n",
    "            'text': 'Time Series Transformer Multi-Scale Clustering Analysis',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 16}\n",
    "        },\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Perform cluster distribution analysis\n",
    "print(\"üî¨ Starting Time Series Transformer cluster analysis...\")\n",
    "\n",
    "if results_3s:\n",
    "    counts_3s, percentages_3s, summary_3s = analyze_tst_cluster_distribution(results_3s, \"3-second\")\n",
    "\n",
    "if results_30s:\n",
    "    counts_30s, percentages_30s, summary_30s = analyze_tst_cluster_distribution(results_30s, \"30-second\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "if results_3s or results_30s:\n",
    "    print(\"\\nüé® Creating multi-scale distribution plots...\")\n",
    "    distribution_fig = create_tst_distribution_plots(results_3s, results_30s)\n",
    "    distribution_fig.show()\n",
    "    print(\"‚úÖ Cluster distribution analysis completed\")\n",
    "else:\n",
    "    print(\"‚ùå No results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d02c58",
   "metadata": {},
   "source": [
    "# 3. Continuous Duration Analysis\n",
    "\n",
    "Analyze the temporal continuity and stability of detected sleep clusters to understand sleep architecture patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6449706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONTINUOUS DURATION AND SLEEP ARCHITECTURE ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_continuous_durations(results, analysis_type):\n",
    "    \"\"\"\n",
    "    Analyze continuous sleep cluster segments to understand sleep architecture.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): TST clustering results\n",
    "        analysis_type (str): Type of analysis (e.g., \"3-second\", \"30-second\")\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (segments_dataframe, duration_statistics, architecture_summary)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results or 'csv' not in results:\n",
    "        print(f\"‚ùå No valid results for {analysis_type} duration analysis\")\n",
    "        return None, None, None\n",
    "    \n",
    "    df = results['csv'].copy()\n",
    "    window_duration = df['end_time_sec'].iloc[0] - df['start_time_sec'].iloc[0]\n",
    "    \n",
    "    print(f\"üèóÔ∏è  SLEEP ARCHITECTURE ANALYSIS ({analysis_type.upper()})\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Identify continuous segments\n",
    "    segments = []\n",
    "    current_cluster = df['predicted_cluster'].iloc[0]\n",
    "    segment_start_idx = 0\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        if df['predicted_cluster'].iloc[i] != current_cluster:\n",
    "            # End of current continuous segment\n",
    "            segment_duration = (i - segment_start_idx) * window_duration\n",
    "            \n",
    "            segments.append({\n",
    "                'cluster': current_cluster,\n",
    "                'start_time_sec': df['start_time_sec'].iloc[segment_start_idx],\n",
    "                'end_time_sec': df['end_time_sec'].iloc[i-1],\n",
    "                'duration_sec': segment_duration,\n",
    "                'duration_min': segment_duration / 60,\n",
    "                'duration_hours': segment_duration / 3600,\n",
    "                'num_windows': i - segment_start_idx,\n",
    "                'start_window_idx': segment_start_idx,\n",
    "                'end_window_idx': i - 1\n",
    "            })\n",
    "            \n",
    "            # Start tracking new segment\n",
    "            current_cluster = df['predicted_cluster'].iloc[i]\n",
    "            segment_start_idx = i\n",
    "    \n",
    "    # Add the final segment\n",
    "    final_duration = (len(df) - segment_start_idx) * window_duration\n",
    "    segments.append({\n",
    "        'cluster': current_cluster,\n",
    "        'start_time_sec': df['start_time_sec'].iloc[segment_start_idx],  \n",
    "        'end_time_sec': df['end_time_sec'].iloc[-1],\n",
    "        'duration_sec': final_duration,\n",
    "        'duration_min': final_duration / 60,\n",
    "        'duration_hours': final_duration / 3600,\n",
    "        'num_windows': len(df) - segment_start_idx,\n",
    "        'start_window_idx': segment_start_idx,\n",
    "        'end_window_idx': len(df) - 1\n",
    "    })\n",
    "    \n",
    "    segments_df = pd.DataFrame(segments)\n",
    "    \n",
    "    print(f\"Sleep Architecture Overview:\")\n",
    "    print(f\"  Total continuous segments: {len(segments_df):,}\")\n",
    "    print(f\"  Average segment length: {segments_df['duration_min'].mean():.2f} minutes\")\n",
    "    print(f\"  Sleep transitions: {len(segments_df) - 1:,}\")\n",
    "    print(f\"  Recording fragmentation: {len(segments_df) / (len(df) * window_duration / 3600):.1f} segments/hour\")\n",
    "    \n",
    "    # Calculate statistics by cluster\n",
    "    duration_stats = segments_df.groupby('cluster')['duration_min'].agg([\n",
    "        'count', 'mean', 'median', 'std', 'min', 'max', 'sum'\n",
    "    ]).round(2)\n",
    "    \n",
    "    duration_stats.columns = ['segments', 'mean_duration', 'median_duration', \n",
    "                             'std_duration', 'min_duration', 'max_duration', 'total_duration']\n",
    "    \n",
    "    print(f\"\\nCluster-Specific Architecture (Duration in minutes):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Cluster':<8} {'Segments':<8} {'Mean':<7} {'Median':<7} {'Std':<7} {'Min':<6} {'Max':<7} {'Total':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for cluster in sorted(duration_stats.index):\n",
    "        stats = duration_stats.loc[cluster]\n",
    "        print(f\"{cluster:<8} {stats['segments']:<8.0f} {stats['mean_duration']:<7.1f} \"\n",
    "              f\"{stats['median_duration']:<7.1f} {stats['std_duration']:<7.1f} \"\n",
    "              f\"{stats['min_duration']:<6.1f} {stats['max_duration']:<7.1f} {stats['total_duration']:<8.1f}\")\n",
    "    \n",
    "    # Architecture quality metrics\n",
    "    total_recording_time = segments_df['duration_min'].sum()\n",
    "    mean_segment_duration = segments_df['duration_min'].mean()\n",
    "    segment_stability = segments_df['duration_min'].median()  # Median as stability measure\n",
    "    \n",
    "    # Find most and least stable clusters\n",
    "    most_stable_cluster = duration_stats['mean_duration'].idxmax()\n",
    "    least_stable_cluster = duration_stats['mean_duration'].idxmin()\n",
    "    longest_segment = segments_df.loc[segments_df['duration_min'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nArchitecture Quality Metrics:\")\n",
    "    print(f\"  Most stable cluster: {most_stable_cluster} (avg {duration_stats.loc[most_stable_cluster, 'mean_duration']:.1f}min)\")\n",
    "    print(f\"  Most dynamic cluster: {least_stable_cluster} (avg {duration_stats.loc[least_stable_cluster, 'mean_duration']:.1f}min)\")\n",
    "    print(f\"  Longest single segment: Cluster {longest_segment['cluster']} ({longest_segment['duration_min']:.1f}min)\")\n",
    "    print(f\"  Overall sleep continuity: {segment_stability:.1f}min median segment\")\n",
    "    \n",
    "    architecture_summary = {\n",
    "        'total_segments': len(segments_df),\n",
    "        'mean_segment_duration': mean_segment_duration,\n",
    "        'most_stable_cluster': most_stable_cluster,\n",
    "        'least_stable_cluster': least_stable_cluster,\n",
    "        'longest_segment_duration': longest_segment['duration_min'],\n",
    "        'segment_stability': segment_stability\n",
    "    }\n",
    "    \n",
    "    return segments_df, duration_stats, architecture_summary\n",
    "\n",
    "def create_duration_analysis_plots(segments_3s, segments_30s, stats_3s, stats_30s):\n",
    "    \"\"\"Create comprehensive duration analysis visualizations.\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Segment Duration Distributions',\n",
    "            'Architecture Stability by Cluster',\n",
    "            'Segment Count vs Duration (3s)',\n",
    "            'Segment Count vs Duration (30s)'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{'type': 'histogram'}, {'type': 'bar'}],\n",
    "            [{'type': 'scatter'}, {'type': 'scatter'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Duration distributions\n",
    "    if segments_3s is not None:\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=segments_3s['duration_min'],\n",
    "            name='3s Segments',\n",
    "            opacity=0.7,\n",
    "            nbinsx=30\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    if segments_30s is not None:\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=segments_30s['duration_min'],\n",
    "            name='30s Segments',\n",
    "            opacity=0.7,\n",
    "            nbinsx=30\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    # Stability comparison\n",
    "    if stats_3s is not None and stats_30s is not None:\n",
    "        clusters = sorted(set(stats_3s.index) | set(stats_30s.index))\n",
    "        \n",
    "        mean_3s = [stats_3s.loc[c, 'mean_duration'] if c in stats_3s.index else 0 for c in clusters]\n",
    "        mean_30s = [stats_30s.loc[c, 'mean_duration'] if c in stats_30s.index else 0 for c in clusters]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=clusters,\n",
    "            y=mean_3s,\n",
    "            name='3s Mean Duration',\n",
    "            opacity=0.7\n",
    "        ), row=1, col=2)\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=clusters,\n",
    "            y=mean_30s,\n",
    "            name='30s Mean Duration',\n",
    "            opacity=0.7\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    # Scatter plots for segment analysis\n",
    "    if segments_3s is not None:\n",
    "        segment_counts_3s = segments_3s['cluster'].value_counts()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=segment_counts_3s.values,\n",
    "            y=[segments_3s[segments_3s['cluster']==c]['duration_min'].mean() for c in segment_counts_3s.index],\n",
    "            mode='markers+text',\n",
    "            text=[f'C{c}' for c in segment_counts_3s.index],\n",
    "            textposition='top center',\n",
    "            name='3s Analysis',\n",
    "            marker=dict(size=10, opacity=0.7)\n",
    "        ), row=2, col=1)\n",
    "    \n",
    "    if segments_30s is not None:\n",
    "        segment_counts_30s = segments_30s['cluster'].value_counts()\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=segment_counts_30s.values,\n",
    "            y=[segments_30s[segments_30s['cluster']==c]['duration_min'].mean() for c in segment_counts_30s.index],\n",
    "            mode='markers+text',\n",
    "            text=[f'C{c}' for c in segment_counts_30s.index],\n",
    "            textposition='top center',\n",
    "            name='30s Analysis',\n",
    "            marker=dict(size=10, opacity=0.7)\n",
    "        ), row=2, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        title={\n",
    "            'text': 'Sleep Architecture - Continuous Duration Analysis',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 16}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Duration (minutes)', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Frequency', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Cluster', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Mean Duration (minutes)', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='Number of Segments', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Mean Duration (minutes)', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Number of Segments', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Mean Duration (minutes)', row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Perform continuous duration analysis\n",
    "print(\"üèóÔ∏è  Starting sleep architecture analysis...\")\n",
    "\n",
    "segments_3s, stats_3s, summary_3s = None, None, None\n",
    "segments_30s, stats_30s, summary_30s = None, None, None\n",
    "\n",
    "if results_3s:\n",
    "    segments_3s, stats_3s, summary_3s = analyze_continuous_durations(results_3s, \"3-second\")\n",
    "\n",
    "if results_30s:\n",
    "    segments_30s, stats_30s, summary_30s = analyze_continuous_durations(results_30s, \"30-second\")\n",
    "\n",
    "# Create comprehensive visualization\n",
    "if segments_3s is not None or segments_30s is not None:\n",
    "    print(\"\\nüé® Creating duration analysis visualizations...\")\n",
    "    duration_fig = create_duration_analysis_plots(segments_3s, segments_30s, stats_3s, stats_30s)\n",
    "    duration_fig.show()\n",
    "    print(\"‚úÖ Sleep architecture analysis completed\")\n",
    "else:\n",
    "    print(\"‚ùå No segment data available for duration analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0600eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# FREQUENCY DOMAIN ANALYSIS AND SPECTRAL CHARACTERISTICS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_cluster_frequency_characteristics(segments_df, results, analysis_type):\n",
    "    \"\"\"\n",
    "    Analyze frequency domain characteristics of different sleep clusters.\n",
    "    \n",
    "    Args:\n",
    "        segments_df (pd.DataFrame): Continuous segments data\n",
    "        results (dict): TST clustering results\n",
    "        analysis_type (str): Type of analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Frequency analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if segments_df is None or results is None:\n",
    "        print(f\"‚ùå No data available for {analysis_type} frequency analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üìä FREQUENCY CHARACTERISTICS ANALYSIS ({analysis_type.upper()})\")\n",
    "    print(\"=\" * 58)\n",
    "    \n",
    "    # Extract key frequency metrics\n",
    "    frequency_metrics = {}\n",
    "    \n",
    "    # Cluster-specific frequency patterns\n",
    "    for cluster in sorted(segments_df['cluster'].unique()):\n",
    "        cluster_segments = segments_df[segments_df['cluster'] == cluster]\n",
    "        \n",
    "        total_time = cluster_segments['duration_min'].sum()\n",
    "        avg_segment_duration = cluster_segments['duration_min'].mean()\n",
    "        dominant_frequency = 1 / (avg_segment_duration * 60)  # Hz equivalent\n",
    "        \n",
    "        frequency_metrics[cluster] = {\n",
    "            'total_time_min': total_time,\n",
    "            'avg_segment_duration_min': avg_segment_duration,\n",
    "            'segment_count': len(cluster_segments),\n",
    "            'dominant_frequency_hz': dominant_frequency,\n",
    "            'time_percentage': (total_time / segments_df['duration_min'].sum()) * 100\n",
    "        }\n",
    "    \n",
    "    print(f\"Cluster Frequency Characteristics:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Cluster':<8} {'Time%':<6} {'Segments':<9} {'Avg Dur(min)':<12} {'Dom Freq(Hz)':<12}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for cluster in sorted(frequency_metrics.keys()):\n",
    "        metrics = frequency_metrics[cluster]\n",
    "        print(f\"{cluster:<8} {metrics['time_percentage']:<6.1f} {metrics['segment_count']:<9} \"\n",
    "              f\"{metrics['avg_segment_duration_min']:<12.2f} {metrics['dominant_frequency_hz']:<12.4f}\")\n",
    "    \n",
    "    # Temporal frequency analysis (transitions per hour)\n",
    "    total_recording_hours = segments_df['duration_min'].sum() / 60\n",
    "    transition_frequency = len(segments_df) / total_recording_hours\n",
    "    \n",
    "    print(f\"\\nTemporal Dynamics:\")\n",
    "    print(f\"  Total recording time: {total_recording_hours:.2f} hours\")\n",
    "    print(f\"  Segment transitions: {len(segments_df):,}\")\n",
    "    print(f\"  Transition frequency: {transition_frequency:.2f} transitions/hour\")\n",
    "    print(f\"  Sleep fragmentation index: {transition_frequency / 10:.2f} (normalized)\")\n",
    "    \n",
    "    # Identify frequency bands based on segment durations\n",
    "    short_segments = segments_df[segments_df['duration_min'] < 5]\n",
    "    medium_segments = segments_df[(segments_df['duration_min'] >= 5) & (segments_df['duration_min'] < 20)]\n",
    "    long_segments = segments_df[segments_df['duration_min'] >= 20]\n",
    "    \n",
    "    print(f\"\\nSegment Duration Distribution:\")\n",
    "    print(f\"  Short segments (<5min): {len(short_segments):,} ({len(short_segments)/len(segments_df)*100:.1f}%)\")\n",
    "    print(f\"  Medium segments (5-20min): {len(medium_segments):,} ({len(medium_segments)/len(segments_df)*100:.1f}%)\")\n",
    "    print(f\"  Long segments (>20min): {len(long_segments):,} ({len(long_segments)/len(segments_df)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'frequency_metrics': frequency_metrics,\n",
    "        'transition_frequency': transition_frequency,\n",
    "        'segment_distribution': {\n",
    "            'short': len(short_segments),\n",
    "            'medium': len(medium_segments),\n",
    "            'long': len(long_segments)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def create_frequency_analysis_plots(freq_results_3s, freq_results_30s):\n",
    "    \"\"\"Create frequency domain analysis visualizations.\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Time Distribution by Cluster',\n",
    "            'Transition Frequency Comparison',\n",
    "            'Segment Duration Categories (3s)',\n",
    "            'Segment Duration Categories (30s)'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'bar'}],\n",
    "            [{'type': 'pie'}, {'type': 'pie'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Time distribution by cluster\n",
    "    if freq_results_3s and freq_results_30s:\n",
    "        clusters_3s = sorted(freq_results_3s['frequency_metrics'].keys())\n",
    "        clusters_30s = sorted(freq_results_30s['frequency_metrics'].keys())\n",
    "        \n",
    "        time_pct_3s = [freq_results_3s['frequency_metrics'][c]['time_percentage'] for c in clusters_3s]\n",
    "        time_pct_30s = [freq_results_30s['frequency_metrics'][c]['time_percentage'] for c in clusters_30s]\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=clusters_3s,\n",
    "            y=time_pct_3s,\n",
    "            name='3s Analysis',\n",
    "            opacity=0.7\n",
    "        ), row=1, col=1)\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=clusters_30s,\n",
    "            y=time_pct_30s,\n",
    "            name='30s Analysis',\n",
    "            opacity=0.7\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    # Transition frequency comparison\n",
    "    if freq_results_3s and freq_results_30s:\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=['3-second Analysis', '30-second Analysis'],\n",
    "            y=[freq_results_3s['transition_frequency'], freq_results_30s['transition_frequency']],\n",
    "            name='Transition Frequency',\n",
    "            marker_color=['lightblue', 'lightcoral']\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    # Segment duration category pie charts\n",
    "    if freq_results_3s:\n",
    "        seg_dist_3s = freq_results_3s['segment_distribution']\n",
    "        fig.add_trace(go.Pie(\n",
    "            labels=['Short (<5min)', 'Medium (5-20min)', 'Long (>20min)'],\n",
    "            values=[seg_dist_3s['short'], seg_dist_3s['medium'], seg_dist_3s['long']],\n",
    "            name='3s Duration Categories'\n",
    "        ), row=2, col=1)\n",
    "    \n",
    "    if freq_results_30s:\n",
    "        seg_dist_30s = freq_results_30s['segment_distribution']\n",
    "        fig.add_trace(go.Pie(\n",
    "            labels=['Short (<5min)', 'Medium (5-20min)', 'Long (>20min)'],\n",
    "            values=[seg_dist_30s['short'], seg_dist_30s['medium'], seg_dist_30s['long']],\n",
    "            name='30s Duration Categories'\n",
    "        ), row=2, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=700,\n",
    "        title={\n",
    "            'text': 'Frequency Domain and Temporal Dynamics Analysis',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 16}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Cluster', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Time Percentage (%)', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Analysis Type', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='Transitions/Hour', row=1, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Perform frequency analysis\n",
    "print(\"üìä Starting frequency characteristics analysis...\")\n",
    "\n",
    "freq_results_3s = None\n",
    "freq_results_30s = None\n",
    "\n",
    "if segments_3s is not None:\n",
    "    freq_results_3s = analyze_cluster_frequency_characteristics(segments_3s, results_3s, \"3-second\")\n",
    "\n",
    "if segments_30s is not None:\n",
    "    freq_results_30s = analyze_cluster_frequency_characteristics(segments_30s, results_30s, \"30-second\")\n",
    "\n",
    "# Create frequency analysis visualizations\n",
    "if freq_results_3s or freq_results_30s:\n",
    "    print(\"\\nüé® Creating frequency analysis visualizations...\")\n",
    "    freq_fig = create_frequency_analysis_plots(freq_results_3s, freq_results_30s)\n",
    "    freq_fig.show()\n",
    "    print(\"‚úÖ Frequency characteristics analysis completed\")\n",
    "else:\n",
    "    print(\"‚ùå No frequency data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TEMPORAL DYNAMICS AND TRANSITION ANALYSIS\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_temporal_transitions(segments_df, results, analysis_type):\n",
    "    \"\"\"\n",
    "    Analyze temporal transitions between sleep clusters.\n",
    "    \n",
    "    Args:\n",
    "        segments_df (pd.DataFrame): Continuous segments data\n",
    "        results (dict): TST clustering results\n",
    "        analysis_type (str): Type of analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Transition analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if segments_df is None or results is None:\n",
    "        print(f\"‚ùå No data available for {analysis_type} transition analysis\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"üîÑ TEMPORAL TRANSITION ANALYSIS ({analysis_type.upper()})\")\n",
    "    print(\"=\" * 52)\n",
    "    \n",
    "    # Create transition matrix\n",
    "    clusters = sorted(segments_df['cluster'].unique())\n",
    "    transition_matrix = np.zeros((len(clusters), len(clusters)))\n",
    "    cluster_to_idx = {cluster: i for i, cluster in enumerate(clusters)}\n",
    "    \n",
    "    # Count transitions\n",
    "    transitions = []\n",
    "    for i in range(len(segments_df) - 1):\n",
    "        from_cluster = segments_df.iloc[i]['cluster']\n",
    "        to_cluster = segments_df.iloc[i + 1]['cluster']\n",
    "        \n",
    "        transitions.append({\n",
    "            'from_cluster': from_cluster,\n",
    "            'to_cluster': to_cluster,\n",
    "            'from_duration': segments_df.iloc[i]['duration_min'],\n",
    "            'to_duration': segments_df.iloc[i + 1]['duration_min'],\n",
    "            'transition_time': segments_df.iloc[i]['end_time_sec'] / 3600  # hours\n",
    "        })\n",
    "        \n",
    "        from_idx = cluster_to_idx[from_cluster]\n",
    "        to_idx = cluster_to_idx[to_cluster]\n",
    "        transition_matrix[from_idx, to_idx] += 1\n",
    "    \n",
    "    transitions_df = pd.DataFrame(transitions)\n",
    "    \n",
    "    # Calculate transition probabilities\n",
    "    transition_probs = transition_matrix.copy()\n",
    "    for i in range(len(clusters)):\n",
    "        row_sum = transition_matrix[i, :].sum()  \n",
    "        if row_sum > 0:\n",
    "            transition_probs[i, :] = transition_matrix[i, :] / row_sum\n",
    "    \n",
    "    print(f\"Transition Matrix (Raw Counts):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'From\\\\To':<8}\", end=\"\")\n",
    "    for cluster in clusters:\n",
    "        print(f\"{cluster:>8}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, from_cluster in enumerate(clusters):\n",
    "        print(f\"{from_cluster:<8}\", end=\"\")\n",
    "        for j in range(len(clusters)):\n",
    "            print(f\"{transition_matrix[i, j]:>8.0f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"\\nTransition Probabilities:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'From\\\\To':<8}\", end=\"\")\n",
    "    for cluster in clusters:\n",
    "        print(f\"{cluster:>8}\", end=\"\")\n",
    "    print()\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, from_cluster in enumerate(clusters):\n",
    "        print(f\"{from_cluster:<8}\", end=\"\")\n",
    "        for j in range(len(clusters)):\n",
    "            print(f\"{transition_probs[i, j]:>8.2f}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Analyze transition patterns\n",
    "    most_common_transitions = transitions_df.groupby(['from_cluster', 'to_cluster']).size().sort_values(ascending=False)\n",
    "    \n",
    "    print(f\"\\nMost Common Transitions:\")\n",
    "    print(\"-\" * 30)\n",
    "    for (from_c, to_c), count in most_common_transitions.head(5).items():\n",
    "        prob = count / len(transitions_df)\n",
    "        print(f\"  {from_c} ‚Üí {to_c}: {count:,} transitions ({prob:.1%})\")\n",
    "    \n",
    "    # Stability analysis\n",
    "    diagonal_sum = np.diag(transition_matrix).sum()\n",
    "    total_transitions = transition_matrix.sum()\n",
    "    stability_index = diagonal_sum / total_transitions if total_transitions > 0 else 0\n",
    "    \n",
    "    print(f\"\\nStability Metrics:\")\n",
    "    print(f\"  Total transitions: {total_transitions:.0f}\")\n",
    "    print(f\"  Self-transitions (stability): {diagonal_sum:.0f}\")\n",
    "    print(f\"  Stability index: {stability_index:.3f}\")\n",
    "    print(f\"  Average transitions per segment: {total_transitions / len(segments_df):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'transition_matrix': transition_matrix,\n",
    "        'transition_probs': transition_probs,\n",
    "        'transitions_df': transitions_df,\n",
    "        'clusters': clusters,\n",
    "        'stability_index': stability_index,\n",
    "        'most_common_transitions': most_common_transitions\n",
    "    }\n",
    "\n",
    "def create_transition_analysis_plots(trans_results_3s, trans_results_30s):\n",
    "    \"\"\"Create temporal transition analysis visualizations.\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Transition Matrix Heatmap (3s)',\n",
    "            'Transition Matrix Heatmap (30s)', \n",
    "            'Transition Probability Comparison',\n",
    "            'Stability Index Comparison'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{'type': 'heatmap'}, {'type': 'heatmap'}],\n",
    "            [{'type': 'scatter'}, {'type': 'bar'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Transition matrix heatmaps\n",
    "    if trans_results_3s:\n",
    "        clusters_3s = trans_results_3s['clusters']\n",
    "        matrix_3s = trans_results_3s['transition_probs']\n",
    "        \n",
    "        fig.add_trace(go.Heatmap(\n",
    "            z=matrix_3s,\n",
    "            x=clusters_3s,\n",
    "            y=clusters_3s,\n",
    "            colorscale='Blues',\n",
    "            name='3s Transitions'\n",
    "        ), row=1, col=1)\n",
    "    \n",
    "    if trans_results_30s:\n",
    "        clusters_30s = trans_results_30s['clusters']\n",
    "        matrix_30s = trans_results_30s['transition_probs']\n",
    "        \n",
    "        fig.add_trace(go.Heatmap(\n",
    "            z=matrix_30s, \n",
    "            x=clusters_30s,\n",
    "            y=clusters_30s,\n",
    "            colorscale='Reds',\n",
    "            name='30s Transitions'\n",
    "        ), row=1, col=2)\n",
    "    \n",
    "    # Transition probability comparison\n",
    "    if trans_results_3s and trans_results_30s:\n",
    "        # Get common clusters and their transition probabilities\n",
    "        common_clusters = set(trans_results_3s['clusters']) & set(trans_results_30s['clusters'])\n",
    "        \n",
    "        for from_cluster in sorted(common_clusters):\n",
    "            for to_cluster in sorted(common_clusters):\n",
    "                if from_cluster != to_cluster:  # Skip self-transitions for clarity\n",
    "                    from_idx_3s = trans_results_3s['clusters'].index(from_cluster)\n",
    "                    to_idx_3s = trans_results_3s['clusters'].index(to_cluster)\n",
    "                    prob_3s = trans_results_3s['transition_probs'][from_idx_3s, to_idx_3s]\n",
    "                    \n",
    "                    from_idx_30s = trans_results_30s['clusters'].index(from_cluster)\n",
    "                    to_idx_30s = trans_results_30s['clusters'].index(to_cluster)\n",
    "                    prob_30s = trans_results_30s['transition_probs'][from_idx_30s, to_idx_30s]\n",
    "                    \n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=[prob_3s],\n",
    "                        y=[prob_30s],\n",
    "                        mode='markers+text',\n",
    "                        text=[f'{from_cluster}‚Üí{to_cluster}'],\n",
    "                        textposition='top center',\n",
    "                        name=f'{from_cluster}‚Üí{to_cluster}',\n",
    "                        showlegend=False\n",
    "                    ), row=2, col=1)\n",
    "    \n",
    "    # Stability comparison\n",
    "    if trans_results_3s and trans_results_30s:\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=['3-second Analysis', '30-second Analysis'],\n",
    "            y=[trans_results_3s['stability_index'], trans_results_30s['stability_index']],\n",
    "            name='Stability Index',\n",
    "            marker_color=['lightblue', 'lightcoral']\n",
    "        ), row=2, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title={\n",
    "            'text': 'Temporal Dynamics - Transition Analysis',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 16}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='To Cluster', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='From Cluster', row=1, col=1)\n",
    "    fig.update_xaxes(title_text='To Cluster', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='From Cluster', row=1, col=2)\n",
    "    fig.update_xaxes(title_text='3s Transition Probability', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='30s Transition Probability', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Analysis Type', row=2, col=2)\n",
    "    fig.update_yaxes(title_text='Stability Index', row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Perform temporal transition analysis\n",
    "print(\"üîÑ Starting temporal transition analysis...\")\n",
    "\n",
    "trans_results_3s = None\n",
    "trans_results_30s = None\n",
    "\n",
    "if segments_3s is not None:\n",
    "    trans_results_3s = analyze_temporal_transitions(segments_3s, results_3s, \"3-second\")\n",
    "\n",
    "if segments_30s is not None:\n",
    "    trans_results_30s = analyze_temporal_transitions(segments_30s, results_30s, \"30-second\")\n",
    "\n",
    "# Create transition analysis visualizations\n",
    "if trans_results_3s or trans_results_30s:\n",
    "    print(\"\\nüé® Creating transition analysis visualizations...\")\n",
    "    trans_fig = create_transition_analysis_plots(trans_results_3s, trans_results_30s)\n",
    "    trans_fig.show()\n",
    "    print(\"‚úÖ Temporal transition analysis completed\")\n",
    "else:\n",
    "    print(\"‚ùå No transition data available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23969952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# COMPREHENSIVE ANALYSIS SUMMARY AND CLINICAL INSIGHTS\n",
    "# ==============================================================================\n",
    "\n",
    "def generate_comprehensive_summary(results_3s, results_30s, segments_3s, segments_30s, \n",
    "                                 freq_results_3s, freq_results_30s, \n",
    "                                 trans_results_3s, trans_results_30s):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive summary of TST clustering analysis results.\n",
    "    \n",
    "    Args:\n",
    "        Various analysis results from different temporal scales\n",
    "        \n",
    "    Returns:\n",
    "        dict: Comprehensive analysis summary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üìã COMPREHENSIVE TST CLUSTERING ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 58)\n",
    "    \n",
    "    summary = {\n",
    "        'temporal_scale_comparison': {},\n",
    "        'cluster_characteristics': {},\n",
    "        'clinical_insights': {},\n",
    "        'methodological_findings': {}\n",
    "    }\n",
    "    \n",
    "    # Temporal Scale Comparison\n",
    "    print(\"\\nüîç TEMPORAL SCALE COMPARISON\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    if results_3s and results_30s:\n",
    "        # Basic comparison\n",
    "        clusters_3s = len(set(results_3s['csv']['predicted_cluster'])) if results_3s.get('csv') is not None else 0\n",
    "        clusters_30s = len(set(results_30s['csv']['predicted_cluster'])) if results_30s.get('csv') is not None else 0\n",
    "        \n",
    "        windows_3s = len(results_3s['csv']) if results_3s.get('csv') is not None else 0\n",
    "        windows_30s = len(results_30s['csv']) if results_30s.get('csv') is not None else 0\n",
    "        \n",
    "        print(f\"3-second Analysis:\")\n",
    "        print(f\"  ‚Ä¢ Total windows: {windows_3s:,}\")\n",
    "        print(f\"  ‚Ä¢ Unique clusters: {clusters_3s}\")\n",
    "        print(f\"  ‚Ä¢ Temporal resolution: High (fine-grained micro-patterns)\")\n",
    "        \n",
    "        print(f\"\\n30-second Analysis:\")\n",
    "        print(f\"  ‚Ä¢ Total windows: {windows_30s:,}\")\n",
    "        print(f\"  ‚Ä¢ Unique clusters: {clusters_30s}\")\n",
    "        print(f\"  ‚Ä¢ Temporal resolution: Standard (macro sleep architecture)\")\n",
    "        \n",
    "        # Resolution ratio\n",
    "        resolution_ratio = windows_3s / windows_30s if windows_30s > 0 else 0\n",
    "        print(f\"\\nTemporal Resolution Ratio: {resolution_ratio:.1f}:1 (3s:30s)\")\n",
    "        \n",
    "        summary['temporal_scale_comparison'] = {\n",
    "            'windows_3s': windows_3s,\n",
    "            'windows_30s': windows_30s,\n",
    "            'clusters_3s': clusters_3s,\n",
    "            'clusters_30s': clusters_30s,\n",
    "            'resolution_ratio': resolution_ratio\n",
    "        }\n",
    "    \n",
    "    # Cluster Characteristics Analysis\n",
    "    print(f\"\\nüß† CLUSTER CHARACTERISTICS SUMMARY\")\n",
    "    print(\"-\" * 38)\n",
    "    \n",
    "    if freq_results_3s and freq_results_30s:\n",
    "        print(\"Time Distribution Patterns:\")\n",
    "        \n",
    "        for scale, freq_results in [(\"3s\", freq_results_3s), (\"30s\", freq_results_30s)]:\n",
    "            print(f\"\\n{scale} Analysis:\")\n",
    "            metrics = freq_results['frequency_metrics']\n",
    "            \n",
    "            # Find dominant and minor clusters\n",
    "            time_percentages = {c: metrics[c]['time_percentage'] for c in metrics}\n",
    "            dominant_cluster = max(time_percentages, key=time_percentages.get)\n",
    "            minor_cluster = min(time_percentages, key=time_percentages.get)\n",
    "            \n",
    "            print(f\"  ‚Ä¢ Dominant cluster: {dominant_cluster} ({time_percentages[dominant_cluster]:.1f}% of recording)\")\n",
    "            print(f\"  ‚Ä¢ Minor cluster: {minor_cluster} ({time_percentages[minor_cluster]:.1f}% of recording)\")\n",
    "            print(f\"  ‚Ä¢ Sleep fragmentation: {freq_results['transition_frequency']:.1f} transitions/hour\")\n",
    "            \n",
    "            summary['cluster_characteristics'][f'{scale}_dominant'] = dominant_cluster\n",
    "            summary['cluster_characteristics'][f'{scale}_fragmentation'] = freq_results['transition_frequency']\n",
    "    \n",
    "    # Architecture Stability Analysis\n",
    "    if trans_results_3s and trans_results_30s:\n",
    "        print(f\"\\nSleep Architecture Stability:\")\n",
    "        stability_3s = trans_results_3s['stability_index']\n",
    "        stability_30s = trans_results_30s['stability_index']\n",
    "        \n",
    "        print(f\"  ‚Ä¢ 3s stability index: {stability_3s:.3f}\")\n",
    "        print(f\"  ‚Ä¢ 30s stability index: {stability_30s:.3f}\")\n",
    "        \n",
    "        stability_difference = abs(stability_3s - stability_30s)\n",
    "        if stability_difference > 0.1:\n",
    "            print(f\"  ‚Ä¢ Scale-dependent stability detected (Œî={stability_difference:.3f})\")\n",
    "        else:\n",
    "            print(f\"  ‚Ä¢ Consistent stability across scales (Œî={stability_difference:.3f})\")\n",
    "        \n",
    "        summary['cluster_characteristics']['stability_comparison'] = {\n",
    "            '3s': stability_3s,\n",
    "            '30s': stability_30s,\n",
    "            'difference': stability_difference\n",
    "        }\n",
    "    \n",
    "    # Clinical Insights\n",
    "    print(f\"\\nüè• CLINICAL INSIGHTS AND INTERPRETATION\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if freq_results_3s and freq_results_30s:\n",
    "        # Sleep quality assessment\n",
    "        frag_3s = freq_results_3s['transition_frequency']\n",
    "        frag_30s = freq_results_30s['transition_frequency']\n",
    "        \n",
    "        print(\"Sleep Quality Assessment:\")\n",
    "        if frag_3s > 60:  # More than 1 transition per minute at 3s scale\n",
    "            print(\"  ‚Ä¢ High micro-fragmentation detected (potential micro-arousals)\")\n",
    "        else:\n",
    "            print(\"  ‚Ä¢ Normal micro-architecture stability\")\n",
    "            \n",
    "        if frag_30s > 10:  # More than 10 transitions per hour at 30s scale\n",
    "            print(\"  ‚Ä¢ Fragmented macro-architecture (potential sleep disorder)\")\n",
    "        else:\n",
    "            print(\"  ‚Ä¢ Stable macro sleep architecture\")\n",
    "        \n",
    "        # Temporal scale insights\n",
    "        scale_ratio = frag_3s / frag_30s if frag_30s > 0 else 0\n",
    "        print(f\"\\nTemporal Scale Insights:\")\n",
    "        print(f\"  ‚Ä¢ Micro/Macro fragmentation ratio: {scale_ratio:.1f}\")\n",
    "        \n",
    "        if scale_ratio > 20:\n",
    "            print(\"  ‚Ä¢ Significant micro-instability with stable macro-architecture\")\n",
    "            print(\"  ‚Ä¢ Suggests: Possible micro-arousal events, breathing disorders\")\n",
    "        elif scale_ratio < 5:\n",
    "            print(\"  ‚Ä¢ Proportional fragmentation across scales\")\n",
    "            print(\"  ‚Ä¢ Suggests: Primary sleep architecture disruption\")\n",
    "        else:\n",
    "            print(\"  ‚Ä¢ Balanced micro/macro sleep dynamics\")\n",
    "        \n",
    "        summary['clinical_insights'] = {\n",
    "            'micro_fragmentation': frag_3s > 60,\n",
    "            'macro_fragmentation': frag_30s > 10,\n",
    "            'scale_ratio': scale_ratio,\n",
    "            'sleep_quality': 'fragmented' if (frag_3s > 60 or frag_30s > 10) else 'stable'\n",
    "        }\n",
    "    \n",
    "    # Methodological Findings\n",
    "    print(f\"\\nüî¨ METHODOLOGICAL FINDINGS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    print(\"Time Series Transformer Performance:\")\n",
    "    if results_3s and results_30s:\n",
    "        print(\"  ‚Ä¢ Multi-scale clustering successfully implemented\")\n",
    "        print(\"  ‚Ä¢ Temporal resolution affects cluster granularity\")\n",
    "        print(\"  ‚Ä¢ Both scales provide complementary information\")\n",
    "        \n",
    "        if clusters_3s > clusters_30s:\n",
    "            print(f\"  ‚Ä¢ Higher resolution reveals {clusters_3s - clusters_30s} additional micro-patterns\")\n",
    "        \n",
    "        print(f\"\\nRecommendations:\")\n",
    "        print(\"  ‚Ä¢ Use 3s windows for micro-arousal detection\")\n",
    "        print(\"  ‚Ä¢ Use 30s windows for sleep stage classification\")\n",
    "        print(\"  ‚Ä¢ Combine both scales for comprehensive sleep analysis\")\n",
    "        print(\"  ‚Ä¢ Consider clinical context when interpreting fragmentation\")\n",
    "        \n",
    "        summary['methodological_findings'] = {\n",
    "            'multi_scale_effective': True,\n",
    "            'complementary_information': True,\n",
    "            'recommended_use': {\n",
    "                '3s': 'micro-arousal detection',\n",
    "                '30s': 'sleep stage classification'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n‚úÖ Analysis Summary Generated Successfully\")\n",
    "    return summary\n",
    "\n",
    "def create_summary_dashboard(summary, results_3s, results_30s):\n",
    "    \"\"\"Create a comprehensive dashboard summarizing all results.\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=3,\n",
    "        subplot_titles=[\n",
    "            'Temporal Resolution Comparison',\n",
    "            'Sleep Fragmentation Assessment', \n",
    "            'Cluster Time Distribution (3s)',\n",
    "            'Cluster Time Distribution (30s)',\n",
    "            'Architecture Stability',\n",
    "            'Clinical Quality Indicators'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{'type': 'bar'}, {'type': 'scatter'}, {'type': 'pie'}],\n",
    "            [{'type': 'pie'}, {'type': 'bar'}, {'type': 'indicator'}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Add plots based on available data\n",
    "    if summary.get('temporal_scale_comparison'):\n",
    "        comp = summary['temporal_scale_comparison']\n",
    "        \n",
    "        # Resolution comparison\n",
    "        fig.add_trace(go.Bar(\n",
    "            x=['3-second', '30-second'],\n",
    "            y=[comp['windows_3s'], comp['windows_30s']],\n",
    "            name='Total Windows',\n",
    "            marker_color=['lightblue', 'lightcoral']\n",
    "        ), row=1, col=1)\n",
    "        \n",
    "        # Fragmentation assessment\n",
    "        if summary.get('cluster_characteristics'):\n",
    "            char = summary['cluster_characteristics']\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[char['3s_fragmentation'], char['30s_fragmentation']],\n",
    "                y=[3, 30],\n",
    "                mode='markers+text',\n",
    "                text=['3s Scale', '30s Scale'],\n",
    "                textposition='middle right',\n",
    "                marker=dict(size=[15, 15], color=['blue', 'red']),\n",
    "                name='Fragmentation Rate'\n",
    "            ), row=1, col=2)\n",
    "    \n",
    "    # Add clinical quality indicator\n",
    "    if summary.get('clinical_insights'):\n",
    "        clinical = summary['clinical_insights']\n",
    "        quality_score = 100 if clinical['sleep_quality'] == 'stable' else 50\n",
    "        \n",
    "        fig.add_trace(go.Indicator(\n",
    "            mode=\"gauge+number\",\n",
    "            value=quality_score,\n",
    "            title={'text': \"Sleep Quality Score\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [0, 100]},\n",
    "                'bar': {'color': \"darkgreen\" if quality_score > 75 else \"orange\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 50], 'color': \"lightgray\"},\n",
    "                    {'range': [50, 80], 'color': \"gray\"},\n",
    "                    {'range': [80, 100], 'color': \"lightgreen\"}\n",
    "                ]\n",
    "            }\n",
    "        ), row=2, col=3)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title={\n",
    "            'text': 'TST Clustering Analysis - Comprehensive Dashboard',\n",
    "            'x': 0.5,\n",
    "            'font': {'size': 18}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Generate comprehensive analysis summary\n",
    "print(\"üìã Generating comprehensive analysis summary...\")\n",
    "\n",
    "if any([results_3s, results_30s, segments_3s, segments_30s]):\n",
    "    analysis_summary = generate_comprehensive_summary(\n",
    "        results_3s, results_30s, segments_3s, segments_30s,\n",
    "        freq_results_3s, freq_results_30s, \n",
    "        trans_results_3s, trans_results_30s\n",
    "    )\n",
    "    \n",
    "    # Create summary dashboard\n",
    "    print(\"\\nüé® Creating comprehensive dashboard...\")\n",
    "    summary_dashboard = create_summary_dashboard(analysis_summary, results_3s, results_30s)\n",
    "    summary_dashboard.show()\n",
    "    \n",
    "    print(\"\\nüéØ KEY FINDINGS SUMMARY:\")\n",
    "    print(\"=\" * 30)\n",
    "    if analysis_summary.get('clinical_insights'):\n",
    "        clinical = analysis_summary['clinical_insights']\n",
    "        print(f\"Sleep Quality: {clinical['sleep_quality'].upper()}\")\n",
    "        print(f\"Micro-fragmentation: {'DETECTED' if clinical['micro_fragmentation'] else 'NORMAL'}\")\n",
    "        print(f\"Macro-fragmentation: {'DETECTED' if clinical['macro_fragmentation'] else 'NORMAL'}\")\n",
    "        print(f\"Scale Ratio: {clinical['scale_ratio']:.1f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Comprehensive TST clustering analysis completed successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Insufficient data for comprehensive analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c62cf",
   "metadata": {},
   "source": [
    "## 4. Frequency Domain Analysis with Multitaper Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69208504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multitaper_spectrogram(eeg_data, fs, window_length=30, overlap=0.5):\n",
    "    \"\"\"Create multitaper spectrogram of EEG data\"\"\"\n",
    "    if eeg_data is None:\n",
    "        print('No EEG data available for spectrogram analysis')\n",
    "        return None, None, None\n",
    "    \n",
    "    # Parameters for spectrogram\n",
    "    nperseg = int(window_length * fs)\n",
    "    noverlap = int(nperseg * overlap)\n",
    "    \n",
    "    # Compute spectrogram (removed invalid 'method' parameter)\n",
    "    frequencies, times, Sxx = signal.spectrogram(\n",
    "        eeg_data, fs,\n",
    "        window='hann',\n",
    "        nperseg=nperseg,\n",
    "        noverlap=noverlap\n",
    "    )\n",
    "    \n",
    "    return frequencies, times, Sxx\n",
    "\n",
    "# Alternative function using scipy's built-in multitaper method\n",
    "def create_multitaper_psd(eeg_data, fs, window_length=30):\n",
    "    \"\"\"Create multitaper power spectral density\"\"\"\n",
    "    if eeg_data is None:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Use multitaper method for PSD estimation\n",
    "        from scipy import signal\n",
    "        nperseg = int(window_length * fs)\n",
    "        \n",
    "        # Calculate PSD using multitaper method\n",
    "        frequencies, psd = signal.welch(\n",
    "            eeg_data, fs,\n",
    "            window='hann',\n",
    "            nperseg=nperseg,\n",
    "            method='multitaper'\n",
    "        )\n",
    "        \n",
    "        return frequencies, psd\n",
    "    except Exception as e:\n",
    "        print(f'Multitaper PSD calculation failed: {e}')\n",
    "        # Fallback to regular welch method\n",
    "        frequencies, psd = signal.welch(\n",
    "            eeg_data, fs,\n",
    "            window='hann',\n",
    "            nperseg=nperseg\n",
    "        )\n",
    "        return frequencies, psd\n",
    "\n",
    "def analyze_frequency_bands(frequencies, Sxx, cluster_labels, times):\n",
    "    \"\"\"Analyze power in different frequency bands for each cluster\"\"\"\n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {\n",
    "        'Delta (0.5-4 Hz)': (0.5, 4),\n",
    "        'Theta (4-8 Hz)': (4, 8),\n",
    "        'Alpha (8-13 Hz)': (8, 13),\n",
    "        'Beta (13-30 Hz)': (13, 30),\n",
    "        'Gamma (30-50 Hz)': (30, 50)\n",
    "    }\n",
    "    \n",
    "    # Calculate power in each band\n",
    "    band_powers = {}\n",
    "    \n",
    "    for band_name, (low_freq, high_freq) in bands.items():\n",
    "        # Find frequency indices\n",
    "        freq_mask = (frequencies >= low_freq) & (frequencies <= high_freq)\n",
    "        \n",
    "        if np.any(freq_mask):\n",
    "            # Calculate mean power in this band across time\n",
    "            band_power = np.mean(Sxx[freq_mask, :], axis=0)\n",
    "            band_powers[band_name] = band_power\n",
    "        else:\n",
    "            band_powers[band_name] = np.zeros(len(times))\n",
    "    \n",
    "    return band_powers\n",
    "\n",
    "def plot_frequency_analysis(eeg_data, fs, results, title_suffix):\n",
    "    \"\"\"Comprehensive frequency domain analysis\"\"\"\n",
    "    \n",
    "    if eeg_data is None:\n",
    "        print(f'Cannot perform frequency analysis for {title_suffix} - no EEG data')\n",
    "        return\n",
    "    \n",
    "    # Truncate EEG data to match clustering results duration\n",
    "    df = results['csv']\n",
    "    max_time = df['end_time_sec'].max()\n",
    "    eeg_truncated = eeg_data[:int(max_time * fs)]\n",
    "    \n",
    "    print(f'\\n=== Frequency Domain Analysis ({title_suffix}) ===')\n",
    "    print(f'EEG duration: {len(eeg_truncated)/fs/3600:.2f} hours')\n",
    "    print(f'Clustering duration: {max_time/3600:.2f} hours')\n",
    "    \n",
    "    # Create spectrogram (fixed function call)\n",
    "    frequencies, times, Sxx = create_multitaper_spectrogram(eeg_truncated, fs)\n",
    "    \n",
    "    if frequencies is None:\n",
    "        return\n",
    "    \n",
    "    # Convert power to dB\n",
    "    Sxx_db = 10 * np.log10(Sxx + 1e-12)\n",
    "    \n",
    "    # Create cluster labels aligned with spectrogram times\n",
    "    cluster_times = []\n",
    "    cluster_labels = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        start_time = row['start_time_sec']\n",
    "        end_time = row['end_time_sec']\n",
    "        cluster = row['cluster_label']\n",
    "        \n",
    "        # Find corresponding time indices in spectrogram\n",
    "        time_mask = (times >= start_time) & (times < end_time)\n",
    "        cluster_times.extend(times[time_mask])\n",
    "        cluster_labels.extend([cluster] * np.sum(time_mask))\n",
    "    \n",
    "    # Analyze frequency bands\n",
    "    band_powers = analyze_frequency_bands(frequencies, Sxx, cluster_labels, times)\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Full spectrogram\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    im1 = ax1.pcolormesh(times/60, frequencies, Sxx_db, shading='gouraud', cmap='viridis')\n",
    "    ax1.set_ylabel('Frequency (Hz)')\n",
    "    ax1.set_title(f'Spectrogram - {title_suffix}')\n",
    "    ax1.set_ylim([0, 50])\n",
    "    plt.colorbar(im1, ax=ax1, label='Power (dB)')\n",
    "    \n",
    "    # 2. Cluster overlay on spectrogram (first 2 hours)\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    time_subset = times <= 7200  # First 2 hours\n",
    "    im2 = ax2.pcolormesh(times[time_subset]/60, frequencies, Sxx_db[:, time_subset], \n",
    "                        shading='gouraud', cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Overlay cluster information\n",
    "    df_subset = df[df['end_time_sec'] <= 7200]\n",
    "    for _, row in df_subset.iterrows():\n",
    "        start_min = row['start_time_sec'] / 60\n",
    "        end_min = row['end_time_sec'] / 60\n",
    "        cluster = int(row['cluster_label'])  # Ensure integer\n",
    "        color_idx = cluster % 10  # Use modulo to stay within colormap range\n",
    "        ax2.axvspan(start_min, end_min, ymin=0.95, ymax=1.0, \n",
    "                   color=plt.cm.Set1(color_idx), alpha=0.8)\n",
    "    \n",
    "    ax2.set_ylabel('Frequency (Hz)')\n",
    "    ax2.set_xlabel('Time (minutes)')\n",
    "    ax2.set_title('Spectrogram with Cluster Overlay (First 2 Hours)')\n",
    "    ax2.set_ylim([0, 50])\n",
    "    plt.colorbar(im2, ax=ax2, label='Power (dB)')\n",
    "    \n",
    "    # 3-6. Frequency band analysis by cluster\n",
    "    band_names = list(band_powers.keys())[:4]  # Show first 4 bands\n",
    "    \n",
    "    for i, band_name in enumerate(band_names):\n",
    "        ax = fig.add_subplot(gs[2 + i//2, i%2])\n",
    "        \n",
    "        # Calculate mean power per cluster\n",
    "        cluster_band_power = {}\n",
    "        for cluster in sorted(df['cluster_label'].unique()):\n",
    "            cluster_mask = np.array(cluster_labels) == cluster\n",
    "            if np.any(cluster_mask):\n",
    "                cluster_times_subset = np.array(cluster_times)[cluster_mask]\n",
    "                time_indices = [np.argmin(np.abs(times - t)) for t in cluster_times_subset]\n",
    "                # Ensure indices are integers and within bounds\n",
    "                time_indices = [int(idx) for idx in time_indices if 0 <= int(idx) < len(band_powers[band_name])]\n",
    "                if time_indices:\n",
    "                    power_values = band_powers[band_name][time_indices]\n",
    "                    cluster_band_power[cluster] = np.mean(power_values)\n",
    "                else:\n",
    "                    cluster_band_power[cluster] = 0\n",
    "            else:\n",
    "                cluster_band_power[cluster] = 0\n",
    "        \n",
    "        clusters = list(cluster_band_power.keys())\n",
    "        powers = list(cluster_band_power.values())\n",
    "        \n",
    "        # Use safe color indexing\n",
    "        colors_safe = [plt.cm.Set1(int(c) % 10) for c in clusters]\n",
    "        bars = ax.bar(clusters, powers, alpha=0.7, color=colors_safe)\n",
    "        ax.set_title(f'{band_name} Power by Cluster')\n",
    "        ax.set_xlabel('Cluster Label')\n",
    "        ax.set_ylabel('Mean Power')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, power in zip(bars, powers):\n",
    "            if power > 0:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                       f'{power:.2e}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # 7. Power spectral density by cluster\n",
    "    ax7 = fig.add_subplot(gs[3, 2])\n",
    "    \n",
    "    for cluster in sorted(df['cluster_label'].unique()):\n",
    "        cluster_mask = np.array(cluster_labels) == cluster\n",
    "        if np.any(cluster_mask):\n",
    "            cluster_times_subset = np.array(cluster_times)[cluster_mask]\n",
    "            # Fix: Convert time indices to integers and add bounds checking\n",
    "            time_indices = []\n",
    "            for t in cluster_times_subset:\n",
    "                idx = np.argmin(np.abs(times - t))\n",
    "                # Ensure index is integer and within bounds\n",
    "                idx = int(idx)\n",
    "                if 0 <= idx < Sxx.shape[1]:\n",
    "                    time_indices.append(idx)\n",
    "            \n",
    "            if time_indices:  # Only proceed if we have valid indices\n",
    "                mean_psd = np.mean(Sxx[:, time_indices], axis=1)\n",
    "                ax7.semilogy(frequencies, mean_psd, label=f'Cluster {cluster}', alpha=0.8)\n",
    "    \n",
    "    ax7.set_xlabel('Frequency (Hz)')\n",
    "    ax7.set_ylabel('Power Spectral Density')\n",
    "    ax7.set_title('Mean PSD by Cluster')\n",
    "    ax7.set_xlim([0, 50])\n",
    "    ax7.legend()\n",
    "    ax7.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Comprehensive Frequency Analysis - {title_suffix}', fontsize=16, fontweight='bold')\n",
    "    plt.show()\n",
    "    \n",
    "    return frequencies, times, Sxx_db, band_powers\n",
    "\n",
    "# Perform frequency analysis\n",
    "if eeg_data is not None:\n",
    "    freq_results_3s = plot_frequency_analysis(eeg_data, fs, results_3s, '3-second windows')\n",
    "    freq_results_30s = plot_frequency_analysis(eeg_data, fs, results_30s, '30-second windows')\n",
    "else:\n",
    "    print('EEG data not available - skipping frequency analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0181b2",
   "metadata": {},
   "source": [
    "## 5. Interactive Hypnogram Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b6036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_hypnogram(results_3s, results_30s):\n",
    "    \"\"\"Create interactive hypnogram comparison\"\"\"\n",
    "    \n",
    "    # Prepare data for 3s results\n",
    "    df_3s = results_3s['csv'].copy()\n",
    "    df_3s['time_hours'] = df_3s['start_time_sec'] / 3600\n",
    "    df_3s['window_type'] = '3-second'\n",
    "    \n",
    "    # Prepare data for 30s results\n",
    "    df_30s = results_30s['csv'].copy()\n",
    "    df_30s['time_hours'] = df_30s['start_time_sec'] / 3600\n",
    "    df_30s['window_type'] = '30-second'\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=('3-Second Window Clustering', '30-Second Window Clustering'),\n",
    "        vertical_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Color mapping for clusters\n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    # Plot 3s results\n",
    "    for cluster in sorted(df_3s['cluster_label'].unique()):\n",
    "        cluster_data = df_3s[df_3s['cluster_label'] == cluster]\n",
    "        # Fix: Ensure cluster index is integer for color selection\n",
    "        color_idx = int(cluster) % len(colors)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=cluster_data['time_hours'],\n",
    "                y=cluster_data['cluster_label'],\n",
    "                mode='markers',\n",
    "                marker=dict(color=colors[color_idx], size=4),\n",
    "                name=f'3s Cluster {cluster}',\n",
    "                legendgroup=f'3s_{cluster}',\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Plot 30s results (limit to same time range as 3s)\n",
    "    max_time_3s = df_3s['time_hours'].max()\n",
    "    df_30s_subset = df_30s[df_30s['time_hours'] <= max_time_3s]\n",
    "    \n",
    "    for cluster in sorted(df_30s_subset['cluster_label'].unique()):\n",
    "        cluster_data = df_30s_subset[df_30s_subset['cluster_label'] == cluster]\n",
    "        # Fix: Ensure cluster index is integer for color selection\n",
    "        color_idx = int(cluster) % len(colors)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=cluster_data['time_hours'],\n",
    "                y=cluster_data['cluster_label'],\n",
    "                mode='markers',\n",
    "                marker=dict(color=colors[color_idx], size=8, symbol='square'),\n",
    "                name=f'30s Cluster {cluster}',\n",
    "                legendgroup=f'30s_{cluster}',\n",
    "                showlegend=True\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Interactive Hypnogram Comparison: Time Series Transformer + Clustering Results',\n",
    "        height=800,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    \n",
    "    # Update axes\n",
    "    fig.update_xaxes(title_text='Time (hours)', row=2, col=1)\n",
    "    fig.update_yaxes(title_text='Cluster Label', dtick=1)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create interactive hypnogram\n",
    "interactive_fig = create_interactive_hypnogram(results_3s, results_30s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_synchronized_plot(eeg_data, fs, results, title_suffix, start_time_hours, duration_hours):\n",
    "    \"\"\"Create interactive synchronized plot with EEG, spectrogram, and hypnogram with synchronized selection/zooming\"\"\"\n",
    "    \n",
    "    if eeg_data is None:\n",
    "        print('No EEG data available for synchronized plot')\n",
    "        return None\n",
    "    \n",
    "    # Convert time parameters to seconds\n",
    "    start_time_sec = start_time_hours * 3600\n",
    "    duration_sec = duration_hours * 3600\n",
    "    end_time_sec = start_time_sec + duration_sec\n",
    "    \n",
    "    # Extract EEG segment\n",
    "    start_idx = int(start_time_sec * fs)\n",
    "    end_idx = int(end_time_sec * fs)\n",
    "    eeg_segment = eeg_data[start_idx:end_idx]\n",
    "    \n",
    "    # Create time array for EEG\n",
    "    eeg_time = np.linspace(start_time_hours, start_time_hours + duration_hours, len(eeg_segment))\n",
    "    \n",
    "    # Extract clustering results for this time segment\n",
    "    df = results['csv'].copy()\n",
    "    df_segment = df[(df['start_time_sec'] >= start_time_sec) & (df['end_time_sec'] <= end_time_sec)].copy()\n",
    "    df_segment['time_hours'] = df_segment['start_time_sec'] / 3600\n",
    "    \n",
    "    # Create spectrogram for the segment\n",
    "    window_length = 10  # seconds for spectrogram\n",
    "    nperseg = int(window_length * fs)\n",
    "    noverlap = int(nperseg * 0.75)\n",
    "    \n",
    "    frequencies, times_spec, Sxx = signal.spectrogram(\n",
    "        eeg_segment, fs,\n",
    "        window='hann',\n",
    "        nperseg=nperseg,\n",
    "        noverlap=noverlap\n",
    "    )\n",
    "    \n",
    "    # Convert spectrogram time to hours (relative to start)\n",
    "    times_spec_hours = start_time_hours + times_spec / 3600\n",
    "    \n",
    "    # Convert power to dB\n",
    "    Sxx_db = 10 * np.log10(Sxx + 1e-12)\n",
    "    \n",
    "    # Create subplot figure with synchronized axes\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        row_heights=[0.3, 0.4, 0.3],\n",
    "        subplot_titles=(\n",
    "            f'EEG Signal - {title_suffix}',\n",
    "            f'Multitaper Spectrogram - {title_suffix}',\n",
    "            f'Hypnogram - {title_suffix}'\n",
    "        ),\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.08\n",
    "    )\n",
    "    \n",
    "    # 1. EEG Signal Plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=eeg_time,\n",
    "            y=eeg_segment,\n",
    "            mode='lines',\n",
    "            name='EEG Signal',\n",
    "            line=dict(color='blue', width=0.5),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Spectrogram Plot\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            x=times_spec_hours,\n",
    "            y=frequencies,\n",
    "            z=Sxx_db,\n",
    "            colorscale='Viridis',\n",
    "            colorbar=dict(\n",
    "                title='Power (dB)',\n",
    "                x=1.02,\n",
    "                y=0.5,\n",
    "                len=0.4\n",
    "            ),\n",
    "            hovertemplate='Time: %{x:.2f} hrs<br>Frequency: %{y:.1f} Hz<br>Power: %{z:.1f} dB<extra></extra>',\n",
    "            showscale=True\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 3. Hypnogram with cluster transitions\n",
    "    colors = px.colors.qualitative.Set1\n",
    "    \n",
    "    # Create step plot for hypnogram\n",
    "    for cluster in sorted(df_segment['cluster_label'].unique()):\n",
    "        cluster_data = df_segment[df_segment['cluster_label'] == cluster]\n",
    "        \n",
    "        # Create step-like visualization\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        \n",
    "        for _, row in cluster_data.iterrows():\n",
    "            start_hour = row['start_time_sec'] / 3600\n",
    "            end_hour = row['end_time_sec'] / 3600\n",
    "            cluster_val = row['cluster_label']\n",
    "            \n",
    "            # Add points for step function\n",
    "            x_vals.extend([start_hour, end_hour, end_hour])\n",
    "            y_vals.extend([cluster_val, cluster_val, None])  # None creates a break\n",
    "        \n",
    "        color_idx = int(cluster) % len(colors)\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_vals,\n",
    "                y=y_vals,\n",
    "                mode='lines',\n",
    "                line=dict(color=colors[color_idx], width=3),\n",
    "                name=f'Cluster {cluster}',\n",
    "                hovertemplate='Time: %{x:.2f} hrs<br>Cluster: %{y}<extra></extra>',\n",
    "                connectgaps=False\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # Add cluster blocks for better visualization\n",
    "    for _, row in df_segment.iterrows():\n",
    "        start_hour = row['start_time_sec'] / 3600\n",
    "        end_hour = row['end_time_sec'] / 3600\n",
    "        cluster = int(row['cluster_label'])\n",
    "        color_idx = cluster % len(colors)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=[start_hour, end_hour, end_hour, start_hour, start_hour],\n",
    "                y=[cluster-0.4, cluster-0.4, cluster+0.4, cluster+0.4, cluster-0.4],\n",
    "                fill='toself',\n",
    "                fillcolor=colors[color_idx],\n",
    "                opacity=0.3,\n",
    "                line=dict(width=0),\n",
    "                showlegend=False,\n",
    "                hoverinfo='skip'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    # Update layout for synchronized zooming and panning\n",
    "    fig.update_layout(\n",
    "        title=f'Synchronized EEG Analysis - {title_suffix}<br>Time Range: {start_time_hours:.1f} - {start_time_hours + duration_hours:.1f} hours',\n",
    "        height=1000,\n",
    "        hovermode='x unified',\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"right\",\n",
    "            x=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update x-axes\n",
    "    fig.update_xaxes(\n",
    "        title_text='Time (hours)',\n",
    "        row=3, col=1,\n",
    "        rangeslider=dict(visible=True, thickness=0.05),\n",
    "        type='linear'\n",
    "    )\n",
    "    \n",
    "    # Update y-axes\n",
    "    fig.update_yaxes(title_text='Amplitude (¬µV)', row=1, col=1)\n",
    "    fig.update_yaxes(\n",
    "        title_text='Frequency (Hz)',\n",
    "        row=2, col=1,\n",
    "        range=[0, 50]  # Focus on relevant frequency range\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text='Cluster Label',\n",
    "        row=3, col=1,\n",
    "        dtick=1,\n",
    "        range=[-0.5, max(df_segment['cluster_label']) + 0.5]\n",
    "    )\n",
    "    \n",
    "    # Add annotation with statistics\n",
    "    total_windows = len(df_segment)\n",
    "    cluster_counts = df_segment['cluster_label'].value_counts().sort_index()\n",
    "    stats_text = f\"Total windows: {total_windows}<br>\"\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        pct = (count / total_windows) * 100\n",
    "        stats_text += f\"Cluster {cluster}: {count} ({pct:.1f}%)<br>\"\n",
    "    \n",
    "    fig.add_annotation(\n",
    "        text=stats_text,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.02, y=0.98,\n",
    "        showarrow=False,\n",
    "        align=\"left\",\n",
    "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def interactive_segment_analyzer(eeg_data, fs, results_3s, results_30s, start_hours, duration_hours):\n",
    "    \"\"\"Interactive function to analyze different time segments\"\"\"\n",
    "    \n",
    "    print(\"=== Interactive Synchronized EEG/Spectrogram/Hypnogram Analyzer ===\")\n",
    "    print(\"\\nAvailable functions:\")\n",
    "    print(\"1. plot_3s_segment(start_hours, duration_hours) - Analyze 3-second results\")\n",
    "    print(\"2. plot_30s_segment(start_hours, duration_hours) - Analyze 30-second results\")\n",
    "    print(\"3. compare_segments(start_hours, duration_hours) - Compare both window sizes\")\n",
    "    print(\"\\nExample usage:\")\n",
    "    print(\"  fig_3s = plot_3s_segment(0, 2)     # First 2 hours with 3s windows\")\n",
    "    print(\"  fig_30s = plot_30s_segment(4, 1)   # Hour 4-5 with 30s windows\")\n",
    "    print(\"  compare_segments(8, 4)             # Hours 8-12 comparison\")\n",
    "    \n",
    "    def plot_3s_segment(start_hours, duration_hours):\n",
    "        \"\"\"Plot synchronized analysis for 3-second windows\"\"\"\n",
    "        return create_interactive_synchronized_plot(\n",
    "            eeg_data, fs, results_3s, '3-second windows', start_hours, duration_hours\n",
    "        )\n",
    "    \n",
    "    def plot_30s_segment(start_hours, duration_hours):\n",
    "        \"\"\"Plot synchronized analysis for 30-second windows\"\"\"\n",
    "        return create_interactive_synchronized_plot(\n",
    "            eeg_data, fs, results_30s, '30-second windows', start_hours, duration_hours\n",
    "        )\n",
    "    \n",
    "    def compare_segments(start_hours, duration_hours):\n",
    "        \"\"\"Compare both window sizes side by side\"\"\"\n",
    "        fig_3s = plot_3s_segment(start_hours, duration_hours)\n",
    "        fig_30s = plot_30s_segment(start_hours, duration_hours)\n",
    "        \n",
    "        if fig_3s and fig_30s:\n",
    "            print(f\"\\nComparison for time range: {start_hours:.1f} - {start_hours + duration_hours:.1f} hours\")\n",
    "            fig_3s.show()\n",
    "            fig_30s.show()\n",
    "            \n",
    "            # Print comparison statistics\n",
    "            df_3s = results_3s['csv']\n",
    "            df_30s = results_30s['csv']\n",
    "            \n",
    "            start_sec = start_hours * 3600\n",
    "            end_sec = (start_hours + duration_hours) * 3600\n",
    "            \n",
    "            df_3s_seg = df_3s[(df_3s['start_time_sec'] >= start_sec) & (df_3s['end_time_sec'] <= end_sec)]\n",
    "            df_30s_seg = df_30s[(df_30s['start_time_sec'] >= start_sec) & (df_30s['end_time_sec'] <= end_sec)]\n",
    "            \n",
    "            print(f\"\\n3-second windows: {len(df_3s_seg)} windows\")\n",
    "            print(f\"30-second windows: {len(df_30s_seg)} windows\")\n",
    "            print(f\"Resolution ratio: {len(df_3s_seg) / max(len(df_30s_seg), 1):.1f}:1\")\n",
    "            \n",
    "            return fig_3s, fig_30s\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    # Return the functions for interactive use\n",
    "    return plot_3s_segment, plot_30s_segment, compare_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1bd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the interactive analyzer\n",
    "start_hours=12\n",
    "duration_hours=1\n",
    "\n",
    "eeg_data, fs, raw_eeg = load_eeg_data()\n",
    "if eeg_data is not None:\n",
    "    plot_3s_segment, plot_30s_segment, compare_segments = interactive_segment_analyzer(\n",
    "        eeg_data, fs, results_3s, results_30s, start_hours, duration_hours\n",
    "    )\n",
    "    \n",
    "    # Create default plots for the first 2 hours\n",
    "    print(\"\\nCreating default synchronized plots for the first 2 hours...\")\n",
    "    default_fig_3s = plot_3s_segment(start_hours, duration_hours)\n",
    "    default_fig_30s = plot_30s_segment(start_hours, duration_hours)\n",
    "    \n",
    "    if default_fig_3s:\n",
    "        default_fig_3s.show()\n",
    "    if default_fig_30s:\n",
    "        default_fig_30s.show()\n",
    "        \n",
    "else:\n",
    "    print(\"EEG data not available - cannot create synchronized plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fb11b2",
   "metadata": {},
   "source": [
    "## 6. Advanced Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caff769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_cluster_statistics(results, title_suffix):\n",
    "    \"\"\"Perform advanced statistical analysis of clustering results\"\"\"\n",
    "    df = results['csv'].copy()\n",
    "    \n",
    "    print(f'\\n=== Advanced Statistical Analysis ({title_suffix}) ===')\n",
    "    \n",
    "    # 1. Temporal distribution analysis\n",
    "    df['hour'] = (df['start_time_sec'] / 3600).astype(int)\n",
    "    hourly_distribution = df.groupby(['hour', 'cluster_label']).size().unstack(fill_value=0)\n",
    "    \n",
    "    print('\\n1. Hourly Distribution of Clusters:')\n",
    "    print(hourly_distribution)\n",
    "    \n",
    "    # 2. Cluster stability analysis (consecutive same-cluster windows)\n",
    "    stability_scores = []\n",
    "    current_cluster = df['cluster_label'].iloc[0]\n",
    "    current_length = 1\n",
    "    \n",
    "    for i in range(1, len(df)):\n",
    "        if df['cluster_label'].iloc[i] == current_cluster:\n",
    "            current_length += 1\n",
    "        else:\n",
    "            stability_scores.append(current_length)\n",
    "            current_cluster = df['cluster_label'].iloc[i]\n",
    "            current_length = 1\n",
    "    stability_scores.append(current_length)\n",
    "    \n",
    "    print(f'\\n2. Cluster Stability Analysis:')\n",
    "    print(f'   Mean consecutive windows: {np.mean(stability_scores):.2f}')\n",
    "    print(f'   Median consecutive windows: {np.median(stability_scores):.2f}')\n",
    "    print(f'   Max consecutive windows: {np.max(stability_scores)}')\n",
    "    print(f'   Number of segments: {len(stability_scores)}')\n",
    "    \n",
    "    # 3. Circadian rhythm analysis (if data spans multiple hours)\n",
    "    if df['hour'].max() >= 4:  # At least 4 hours of data\n",
    "        print(f'\\n3. Circadian Pattern Analysis:')\n",
    "        for cluster in sorted(df['cluster_label'].unique()):\n",
    "            cluster_hours = df[df['cluster_label'] == cluster]['hour'].values\n",
    "            if len(cluster_hours) > 0:\n",
    "                # Fix: Handle the mode calculation properly\n",
    "                try:\n",
    "                    mode_result = stats.mode(cluster_hours, keepdims=False)\n",
    "                    if hasattr(mode_result, 'mode'):\n",
    "                        peak_hour = mode_result.mode\n",
    "                    else:\n",
    "                        peak_hour = mode_result[0]\n",
    "                    print(f'   Cluster {cluster}: Peak activity at hour {peak_hour}')\n",
    "                except Exception as e:\n",
    "                    # Fallback to manual mode calculation\n",
    "                    from collections import Counter\n",
    "                    hour_counts = Counter(cluster_hours)\n",
    "                    peak_hour = hour_counts.most_common(1)[0][0]\n",
    "                    print(f'   Cluster {cluster}: Peak activity at hour {peak_hour}')\n",
    "    \n",
    "    # 4. Transition analysis\n",
    "    transitions = []\n",
    "    for i in range(1, len(df)):\n",
    "        prev_cluster = df['cluster_label'].iloc[i-1]\n",
    "        curr_cluster = df['cluster_label'].iloc[i]\n",
    "        if prev_cluster != curr_cluster:\n",
    "            transitions.append((prev_cluster, curr_cluster))\n",
    "    \n",
    "    if transitions:\n",
    "        transition_counts = pd.Series(transitions).value_counts()\n",
    "        print(f'\\n4. Most Common Transitions:')\n",
    "        print(transition_counts.head(10))\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'Advanced Statistical Analysis - {title_suffix}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Hourly distribution heatmap\n",
    "    if not hourly_distribution.empty:\n",
    "        sns.heatmap(hourly_distribution.T, annot=True, fmt='d', cmap='YlOrRd', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Hourly Cluster Distribution')\n",
    "        axes[0,0].set_xlabel('Hour of Recording')\n",
    "        axes[0,0].set_ylabel('Cluster Label')\n",
    "    \n",
    "    # Stability distribution\n",
    "    axes[0,1].hist(stability_scores, bins=min(30, len(set(stability_scores))), alpha=0.7, edgecolor='black')\n",
    "    axes[0,1].set_title('Distribution of Consecutive Window Lengths')\n",
    "    axes[0,1].set_xlabel('Consecutive Windows')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].axvline(np.mean(stability_scores), color='red', linestyle='--', label=f'Mean: {np.mean(stability_scores):.1f}')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Cluster proportion over time (sliding window)\n",
    "    window_size = max(100, len(df) // 20)  # Adaptive window size\n",
    "    time_points = []\n",
    "    cluster_props = {c: [] for c in sorted(df['cluster_label'].unique())}\n",
    "    \n",
    "    for i in range(window_size, len(df), window_size//2):\n",
    "        window_data = df.iloc[i-window_size:i]\n",
    "        time_points.append(window_data['start_time_sec'].mean() / 3600)\n",
    "        \n",
    "        for cluster in cluster_props.keys():\n",
    "            prop = (window_data['cluster_label'] == cluster).mean()\n",
    "            cluster_props[cluster].append(prop)\n",
    "    \n",
    "    for cluster, props in cluster_props.items():\n",
    "        if len(props) > 0:\n",
    "            axes[1,0].plot(time_points, props, marker='o', label=f'Cluster {cluster}', alpha=0.7)\n",
    "    \n",
    "    axes[1,0].set_title('Cluster Proportions Over Time')\n",
    "    axes[1,0].set_xlabel('Time (hours)')\n",
    "    axes[1,0].set_ylabel('Proportion')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Autocorrelation of cluster sequence\n",
    "    cluster_sequence = df['cluster_label'].values\n",
    "    max_lag = min(100, len(cluster_sequence) // 4)\n",
    "    \n",
    "    autocorr = []\n",
    "    for lag in range(max_lag):\n",
    "        if lag == 0:\n",
    "            autocorr.append(1.0)\n",
    "        else:\n",
    "            # Fix: Ensure proper array indexing and handle edge cases\n",
    "            if lag < len(cluster_sequence):\n",
    "                corr_val = np.corrcoef(cluster_sequence[:-lag], cluster_sequence[lag:])[0,1]\n",
    "                # Handle NaN values\n",
    "                if np.isnan(corr_val):\n",
    "                    corr_val = 0.0\n",
    "                autocorr.append(corr_val)\n",
    "            else:\n",
    "                autocorr.append(0.0)\n",
    "    \n",
    "    axes[1,1].plot(range(len(autocorr)), autocorr, marker='o', markersize=3)\n",
    "    axes[1,1].set_title('Autocorrelation of Cluster Sequence')\n",
    "    axes[1,1].set_xlabel('Lag (windows)')\n",
    "    axes[1,1].set_ylabel('Autocorrelation')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    axes[1,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "    return {\n",
    "        'hourly_distribution': hourly_distribution,\n",
    "        'stability_scores': stability_scores,\n",
    "        'transitions': transitions,\n",
    "        'autocorr': autocorr\n",
    "    }\n",
    "\n",
    "# Perform advanced statistical analysis\n",
    "stats_3s = advanced_cluster_statistics(results_3s, '3-second windows')\n",
    "stats_30s = advanced_cluster_statistics(results_30s, '30-second windows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb56cc35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
