{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f993ebf",
   "metadata": {},
   "source": [
    "# üß† Hidden Markov Model (HMM) Analysis for Sleep EEG Signals\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "This notebook implements a **GPU-accelerated Hidden Markov Model (HMM)** pipeline for EEG sleep signal analysis. The HMM approach models sleep as a sequence of hidden states (sleep stages) that generate observable EEG patterns, making it particularly suitable for:\n",
    "\n",
    "- **Sleep stage classification** using temporal dependencies\n",
    "- **State transition analysis** between different sleep phases  \n",
    "- **Unsupervised clustering** of sleep patterns\n",
    "- **GPU acceleration** for large-scale EEG datasets\n",
    "\n",
    "## üéØ Research Objectives\n",
    "\n",
    "1. **Signal Preprocessing**: Clean and standardize EEG signals from EDF files\n",
    "2. **HMM Implementation**: Custom GPU-accelerated HMM with TensorFlow\n",
    "3. **State Classification**: Identify hidden sleep states from EEG patterns\n",
    "4. **Performance Analysis**: Compare CPU vs GPU computational efficiency\n",
    "5. **Clinical Application**: Apply to real sleep study data for stage detection\n",
    "\n",
    "---\n",
    "\n",
    "### üè• Clinical Context\n",
    "Hidden Markov Models are particularly effective for sleep analysis because:\n",
    "- Sleep stages follow **sequential patterns** (temporal dependencies)\n",
    "- **State transitions** are probabilistic and clinically meaningful\n",
    "- **Observable features** (EEG) are generated by hidden states (sleep stages)\n",
    "- **Unsupervised approach** doesn't require labeled sleep staging data\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Expected Outcomes\n",
    "- Automated sleep stage classification\n",
    "- State transition probability matrices\n",
    "- Performance comparison between CPU/GPU implementations\n",
    "- Visualization of detected sleep architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576362f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:30:36.962521: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow sees the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Is GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# LIBRARY IMPORTS AND GPU CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "from scipy.signal import detrend\n",
    "from hmmlearn import hmm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU acceleration libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Set TensorFlow logging level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "def check_gpu_availability():\n",
    "    \"\"\"Check and display GPU availability for acceleration.\"\"\"\n",
    "    print(\"üñ•Ô∏è  COMPUTING ENVIRONMENT STATUS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Check TensorFlow devices\n",
    "    physical_devices = tf.config.list_physical_devices()\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    \n",
    "    print(f\"Available devices: {len(physical_devices)}\")\n",
    "    for device in physical_devices:\n",
    "        print(f\"  ‚Ä¢ {device}\")\n",
    "    \n",
    "    if gpu_devices:\n",
    "        print(f\"\\n‚úÖ GPU Acceleration: Available ({len(gpu_devices)} GPU(s))\")\n",
    "        for i, gpu in enumerate(gpu_devices):\n",
    "            print(f\"  ‚Ä¢ GPU {i}: {gpu.name}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  GPU Acceleration: Not available - using CPU\")\n",
    "        return False\n",
    "\n",
    "# Check GPU availability\n",
    "gpu_available = check_gpu_availability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ba66b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth set to True\n"
     ]
    }
   ],
   "source": [
    "# Configure GPU memory management to prevent memory errors\n",
    "def configure_gpu_memory():\n",
    "    \"\"\"Configure GPU memory growth to prevent allocation errors.\"\"\"\n",
    "    \n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Enable memory growth for all GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"‚úÖ GPU memory growth configured successfully\")\n",
    "            \n",
    "            # Set memory limit if needed (optional)\n",
    "            # tf.config.experimental.set_memory_limit(gpus[0], 4096)  # 4GB limit\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ùå GPU configuration error: {e}\")\n",
    "            print(\"Continuing with default GPU settings...\")\n",
    "    else:\n",
    "        print(\"No GPU devices found - skipping GPU configuration\")\n",
    "\n",
    "# Configure GPU memory\n",
    "configure_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b71ec1",
   "metadata": {},
   "source": [
    "## üìÅ Step 1: EEG Data Loading and Validation\n",
    "\n",
    "Load EEG signals from EDF files with comprehensive validation and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/yahia/notebooks/by captain borat/raw/EEG_0_per_hour_2024-03-20 17_12_18.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 44153855  =      0.000 ... 86237.998 secs...\n"
     ]
    }
   ],
   "source": [
    "def load_eeg_data(file_path, channel_selection='auto'):\n",
    "    \"\"\"\n",
    "    Load EEG signal from EDF file with comprehensive validation.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to EDF file\n",
    "        channel_selection (str or list): Channel selection strategy\n",
    "            - 'auto': Automatically select best EEG channel\n",
    "            - 'first': Use first available channel\n",
    "            - list: Specific channel names to load\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (eeg_data, sampling_freq, channel_info, metadata)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üìÅ Loading EEG data from: {file_path}\")\n",
    "        \n",
    "        # Load EDF file with MNE\n",
    "        raw = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "        \n",
    "        # Get sampling frequency\n",
    "        sfreq = raw.info['sfreq']\n",
    "        \n",
    "        # Get channel information\n",
    "        channel_names = raw.ch_names\n",
    "        n_channels = len(channel_names)\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Sampling frequency: {sfreq} Hz\")\n",
    "        print(f\"  ‚Ä¢ Number of channels: {n_channels}\")\n",
    "        print(f\"  ‚Ä¢ Recording duration: {raw.times[-1]:.1f} seconds\")\n",
    "        \n",
    "        # Channel selection logic\n",
    "        if channel_selection == 'auto':\n",
    "            # Priority list for sleep EEG analysis\n",
    "            preferred_channels = ['Fpz-Cz', 'Pz-Oz', 'C3-A2', 'C4-A1', 'F3-A2', 'F4-A1']\n",
    "            selected_channel = None\n",
    "            \n",
    "            for pref_ch in preferred_channels:\n",
    "                if pref_ch in channel_names:\n",
    "                    selected_channel = pref_ch\n",
    "                    break\n",
    "            \n",
    "            if selected_channel is None:\n",
    "                selected_channel = channel_names[0]  # Fallback to first channel\n",
    "                \n",
    "        elif channel_selection == 'first':\n",
    "            selected_channel = channel_names[0]\n",
    "        elif isinstance(channel_selection, list):\n",
    "            selected_channel = channel_selection[0] if channel_selection[0] in channel_names else channel_names[0]\n",
    "        else:\n",
    "            selected_channel = channel_names[0]\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Selected channel: {selected_channel}\")\n",
    "        \n",
    "        # Extract data from selected channel\n",
    "        channel_idx = channel_names.index(selected_channel)\n",
    "        eeg_data = raw.get_data()[channel_idx]\n",
    "        \n",
    "        # Create metadata dictionary\n",
    "        metadata = {\n",
    "            'file_path': file_path,\n",
    "            'selected_channel': selected_channel,\n",
    "            'sampling_freq': sfreq,\n",
    "            'n_samples': len(eeg_data),\n",
    "            'duration_sec': len(eeg_data) / sfreq,\n",
    "            'all_channels': channel_names\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ EEG data loaded successfully\")\n",
    "        print(f\"  ‚Ä¢ Data shape: {eeg_data.shape}\")\n",
    "        print(f\"  ‚Ä¢ Data range: [{eeg_data.min():.2f}, {eeg_data.max():.2f}]\")\n",
    "        \n",
    "        return eeg_data, sfreq, selected_channel, metadata\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        return None, None, None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading EDF file: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "# Load EEG data with multiple fallback paths\n",
    "possible_paths = [\n",
    "    'by captain borat/raw/EEG_0_per_hour_2024-03-20 17_12_18.edf',\n",
    "    'raw data/EEG_0_per_hour_2024-03-20 17_12_18.edf',\n",
    "    'by captain borat/raw/SC4001E0-PSG.edf'\n",
    "]\n",
    "\n",
    "eeg_signal = None\n",
    "for path in possible_paths:\n",
    "    result = load_eeg_data(path, channel_selection='auto')\n",
    "    if result[0] is not None:\n",
    "        eeg_signal, sampling_freq, selected_channel, eeg_metadata = result\n",
    "        break\n",
    "\n",
    "if eeg_signal is None:\n",
    "    print(\"‚ùå No valid EEG file found. Please check file paths.\")\n",
    "else:\n",
    "    print(f\"\\nüìä EEG Data Summary:\")\n",
    "    print(f\"  ‚Ä¢ Signal length: {len(eeg_signal):,} samples\")\n",
    "    print(f\"  ‚Ä¢ Duration: {len(eeg_signal)/sampling_freq:.1f} seconds ({len(eeg_signal)/sampling_freq/3600:.2f} hours)\")\n",
    "    print(f\"  ‚Ä¢ Mean amplitude: {np.mean(eeg_signal):.2f}\")\n",
    "    print(f\"  ‚Ä¢ Std amplitude: {np.std(eeg_signal):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a46d85",
   "metadata": {},
   "source": [
    "## üîß Step 2: Signal Preprocessing Pipeline\n",
    "\n",
    "Comprehensive EEG signal preprocessing including artifact removal, interpolation, and standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_artifacts(signal, sampling_freq, window_duration=10, threshold_factor=3):\n",
    "    \"\"\"\n",
    "    Interpolate artifacts in EEG signal using sliding window approach.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): Input EEG signal\n",
    "        sampling_freq (float): Sampling frequency in Hz\n",
    "        window_duration (float): Window duration in seconds\n",
    "        threshold_factor (float): Threshold multiplier for outlier detection\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Interpolated signal with artifacts removed\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîß Interpolating artifacts...\")\n",
    "    print(f\"  ‚Ä¢ Window duration: {window_duration}s\")\n",
    "    print(f\"  ‚Ä¢ Threshold factor: {threshold_factor}x std\")\n",
    "    \n",
    "    interpolated_signal = signal.copy()\n",
    "    window_size = int(sampling_freq * window_duration)\n",
    "    \n",
    "    # Calculate global statistics for reference\n",
    "    global_std = np.std(signal)\n",
    "    threshold = threshold_factor * global_std\n",
    "    \n",
    "    total_artifacts = 0\n",
    "    n_windows = 0\n",
    "    \n",
    "    # Process signal in sliding windows\n",
    "    for i in range(0, len(signal), window_size):\n",
    "        window_end = min(i + window_size, len(signal))\n",
    "        window = signal[i:window_end]\n",
    "        \n",
    "        if len(window) == 0:\n",
    "            continue\n",
    "            \n",
    "        n_windows += 1\n",
    "        window_mean = np.mean(window)\n",
    "        \n",
    "        # Detect outliers (artifacts)\n",
    "        outliers_mask = np.abs(window - window_mean) > threshold\n",
    "        n_outliers = np.sum(outliers_mask)\n",
    "        \n",
    "        if n_outliers > 0:\n",
    "            total_artifacts += n_outliers\n",
    "            # Replace outliers with window mean (simple interpolation)\n",
    "            interpolated_signal[i:window_end][outliers_mask] = window_mean\n",
    "    \n",
    "    artifact_percentage = (total_artifacts / len(signal)) * 100\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Processed {n_windows} windows\")\n",
    "    print(f\"  ‚Ä¢ Artifacts detected: {total_artifacts:,} samples ({artifact_percentage:.2f}%)\")\n",
    "    print(f\"‚úÖ Artifact interpolation completed\")\n",
    "    \n",
    "    return interpolated_signal\n",
    "\n",
    "def standardize_signal(signal):\n",
    "    \"\"\"\n",
    "    Standardize signal to zero mean and unit variance.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): Input signal\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Standardized signal\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üìä Standardizing signal...\")\n",
    "    \n",
    "    mean_val = np.mean(signal)\n",
    "    std_val = np.std(signal)\n",
    "    \n",
    "    if std_val == 0:\n",
    "        print(\"‚ö†Ô∏è  Warning: Signal has zero variance!\")\n",
    "        return signal - mean_val\n",
    "    \n",
    "    standardized = (signal - mean_val) / std_val\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Original mean: {mean_val:.4f}, std: {std_val:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Standardized mean: {np.mean(standardized):.4f}, std: {np.std(standardized):.4f}\")\n",
    "    print(f\"‚úÖ Signal standardization completed\")\n",
    "    \n",
    "    return standardized\n",
    "\n",
    "def remove_linear_trends(signal):\n",
    "    \"\"\"\n",
    "    Remove linear trends from the signal using scipy detrend.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): Input signal\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Detrended signal\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üìà Removing linear trends...\")\n",
    "    \n",
    "    # Calculate trend before removal\n",
    "    x = np.arange(len(signal))\n",
    "    slope, intercept = np.polyfit(x, signal, 1)\n",
    "    \n",
    "    # Remove trend\n",
    "    detrended = detrend(signal)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Original trend slope: {slope:.6f}\")\n",
    "    print(f\"  ‚Ä¢ Trend intercept: {intercept:.4f}\")\n",
    "    print(f\"‚úÖ Linear trend removal completed\")\n",
    "    \n",
    "    return detrended\n",
    "\n",
    "# Apply preprocessing pipeline\n",
    "if eeg_signal is not None:\n",
    "    print(\"üîÑ Starting EEG preprocessing pipeline...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Interpolate artifacts\n",
    "    eeg_interpolated = interpolate_artifacts(\n",
    "        eeg_signal, \n",
    "        sampling_freq, \n",
    "        window_duration=10, \n",
    "        threshold_factor=3\n",
    "    )\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Step 2: Standardize signal\n",
    "    eeg_standardized = standardize_signal(eeg_interpolated)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Step 3: Remove linear trends\n",
    "    eeg_detrended = remove_linear_trends(eeg_standardized)\n",
    "    \n",
    "    print(f\"\\nüìä Preprocessing Summary:\")\n",
    "    print(f\"  ‚Ä¢ Original signal range: [{eeg_signal.min():.2f}, {eeg_signal.max():.2f}]\")\n",
    "    print(f\"  ‚Ä¢ Final processed range: [{eeg_detrended.min():.2f}, {eeg_detrended.max():.2f}]\")\n",
    "    print(f\"  ‚Ä¢ Processing preserved {len(eeg_detrended):,} samples\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No EEG signal available for preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac2c305",
   "metadata": {},
   "source": [
    "## ü§ñ Step 3: GPU-Accelerated Hidden Markov Model Implementation\n",
    "\n",
    "Custom TensorFlow-based HMM implementation with GPU acceleration for large-scale EEG analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFlowHMM:\n",
    "    \"\"\"\n",
    "    GPU-accelerated Hidden Markov Model implementation using TensorFlow.\n",
    "    \n",
    "    This implementation provides:\n",
    "    - GPU acceleration for large datasets\n",
    "    - Expectation-Maximization (EM) algorithm\n",
    "    - Gaussian emission distributions\n",
    "    - Viterbi decoding for state prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=5, n_iter=100, tol=1e-4, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize GPU-accelerated HMM.\n",
    "        \n",
    "        Args:\n",
    "            n_components (int): Number of hidden states\n",
    "            n_iter (int): Maximum number of EM iterations\n",
    "            tol (float): Convergence tolerance\n",
    "            verbose (bool): Print training progress\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Model parameters (will be initialized during fit)\n",
    "        self.means_ = None\n",
    "        self.covars_ = None\n",
    "        self.transmat_ = None\n",
    "        self.startprob_ = None\n",
    "        self.converged_ = False\n",
    "        self.n_features_ = None\n",
    "\n",
    "print(\"ü§ñ GPU-accelerated HMM class initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec3b876",
   "metadata": {},
   "source": [
    "## üßÆ Step 4: HMM Training Methods\n",
    "\n",
    "Implementation of core HMM algorithms including parameter initialization, EM training, and Viterbi decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11208073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hmm_methods_to_class():\n",
    "    \"\"\"Add core HMM methods to the TensorFlowHMM class.\"\"\"\n",
    "    \n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"Initialize HMM parameters randomly.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_features_ = n_features\n",
    "        \n",
    "        # Initialize means using K-means-like approach\n",
    "        indices = tf.random.uniform([self.n_components], 0, n_samples, dtype=tf.int32)\n",
    "        self.means_ = tf.Variable(tf.gather(X, indices), dtype=tf.float32)\n",
    "        \n",
    "        # Initialize covariances\n",
    "        self.covars_ = tf.Variable(tf.ones([self.n_components, n_features]), dtype=tf.float32)\n",
    "        \n",
    "        # Initialize transition matrix (normalized)\n",
    "        transmat_raw = tf.random.uniform([self.n_components, self.n_components])\n",
    "        self.transmat_ = tf.Variable(tf.nn.softmax(transmat_raw, axis=1))\n",
    "        \n",
    "        # Initialize start probabilities (normalized)\n",
    "        startprob_raw = tf.random.uniform([self.n_components])\n",
    "        self.startprob_ = tf.Variable(tf.nn.softmax(startprob_raw))\n",
    "    \n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"Compute log-likelihood of observations given current parameters.\"\"\"\n",
    "        # Expand dimensions for broadcasting\n",
    "        X_expanded = X[tf.newaxis, :, :]  # [1, n_samples, n_features]\n",
    "        means_expanded = self.means_[:, tf.newaxis, :]  # [n_components, 1, n_features]\n",
    "        covars_expanded = self.covars_[:, tf.newaxis, :]  # [n_components, 1, n_features]\n",
    "        \n",
    "        # Compute squared Mahalanobis distance\n",
    "        diff = X_expanded - means_expanded\n",
    "        mahal_dist = tf.reduce_sum(diff**2 / covars_expanded, axis=2)\n",
    "        \n",
    "        # Compute log-likelihood (Gaussian)\n",
    "        log_likelihood = -0.5 * mahal_dist\n",
    "        log_likelihood -= 0.5 * tf.reduce_sum(tf.math.log(2 * np.pi * covars_expanded), axis=2)\n",
    "        \n",
    "        return log_likelihood\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit HMM using Expectation-Maximization algorithm.\"\"\"\n",
    "        X_tf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        n_samples, n_features = X_tf.shape\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"üéØ Training HMM with {self.n_components} states on {n_samples:,} samples...\")\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X_tf)\n",
    "        \n",
    "        prev_log_likelihood = -np.inf\n",
    "        \n",
    "        for iteration in range(self.n_iter):\n",
    "            # E-step: Compute forward-backward probabilities\n",
    "            log_likelihood_matrix = self._compute_log_likelihood(X_tf)\n",
    "            \n",
    "            # Simple responsibility calculation (could be improved with forward-backward)\n",
    "            log_resp = log_likelihood_matrix + tf.math.log(self.startprob_[:, tf.newaxis])\n",
    "            log_resp = log_resp - tf.reduce_logsumexp(log_resp, axis=0, keepdims=True)\n",
    "            resp = tf.exp(log_resp)\n",
    "            \n",
    "            # M-step: Update parameters\n",
    "            # Update means\n",
    "            resp_sum = tf.reduce_sum(resp, axis=1, keepdims=True)\n",
    "            new_means = tf.matmul(resp, X_tf) / resp_sum\n",
    "            self.means_.assign(new_means)\n",
    "            \n",
    "            # Update covariances\n",
    "            diff = X_tf[tf.newaxis, :, :] - self.means_[:, tf.newaxis, :]\n",
    "            weighted_sq_diff = resp[:, :, tf.newaxis] * (diff ** 2)\n",
    "            new_covars = tf.reduce_sum(weighted_sq_diff, axis=1) / resp_sum\n",
    "            # Add small regularization to prevent singular covariance\n",
    "            new_covars = new_covars + 1e-6\n",
    "            self.covars_.assign(new_covars)\n",
    "            \n",
    "            # Compute current log-likelihood for convergence check\n",
    "            current_log_likelihood = tf.reduce_sum(tf.reduce_logsumexp(log_resp, axis=0))\n",
    "            \n",
    "            if self.verbose and (iteration + 1) % 20 == 0:\n",
    "                print(f\"  Iteration {iteration + 1:3d}: Log-likelihood = {current_log_likelihood:.4f}\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if abs(current_log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                if self.verbose:\n",
    "                    print(f\"‚úÖ Converged after {iteration + 1} iterations\")\n",
    "                self.converged_ = True\n",
    "                break\n",
    "                \n",
    "            prev_log_likelihood = current_log_likelihood\n",
    "        \n",
    "        if not self.converged_ and self.verbose:\n",
    "            print(f\"‚ö†Ô∏è  Did not converge after {self.n_iter} iterations\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict hidden states using Viterbi algorithm (simplified).\"\"\"\n",
    "        X_tf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        log_likelihood = self._compute_log_likelihood(X_tf)\n",
    "        \n",
    "        # Simple maximum likelihood decoding (could be improved with Viterbi)\n",
    "        states = tf.argmax(log_likelihood, axis=0)\n",
    "        return states.numpy()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict state probabilities for each observation.\"\"\"\n",
    "        X_tf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "        log_likelihood = self._compute_log_likelihood(X_tf)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        log_probs = log_likelihood - tf.reduce_logsumexp(log_likelihood, axis=0, keepdims=True)\n",
    "        probs = tf.exp(log_probs)\n",
    "        \n",
    "        return probs.numpy().T  # Transpose to [n_samples, n_components]\n",
    "    \n",
    "    # Add methods to the class\n",
    "    TensorFlowHMM._initialize_parameters = _initialize_parameters\n",
    "    TensorFlowHMM._compute_log_likelihood = _compute_log_likelihood\n",
    "    TensorFlowHMM.fit = fit\n",
    "    TensorFlowHMM.predict = predict\n",
    "    TensorFlowHMM.predict_proba = predict_proba\n",
    "\n",
    "# Add methods to the class\n",
    "add_hmm_methods_to_class()\n",
    "print(\"‚úÖ HMM training methods added to TensorFlowHMM class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff1d6a9",
   "metadata": {},
   "source": [
    "## üöÄ Step 5: HMM Training and State Classification\n",
    "\n",
    "Apply the GPU-accelerated HMM to classify sleep states from preprocessed EEG signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57b838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running HMM on: [LogicalDevice(name='/device:CPU:0', device_type='CPU'), LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-04 15:30:44.166239: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-04 15:30:44.379233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10446 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "def prepare_features_for_hmm(signal, feature_window=512, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Prepare features from EEG signal for HMM training.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): Preprocessed EEG signal\n",
    "        feature_window (int): Window size for feature extraction\n",
    "        overlap (float): Overlap between windows (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Feature matrix [n_windows, n_features]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîß Preparing features for HMM...\")\n",
    "    \n",
    "    step_size = int(feature_window * (1 - overlap))\n",
    "    n_windows = (len(signal) - feature_window) // step_size + 1\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        start_idx = i * step_size\n",
    "        end_idx = start_idx + feature_window\n",
    "        \n",
    "        if end_idx > len(signal):\n",
    "            break\n",
    "            \n",
    "        window = signal[start_idx:end_idx]\n",
    "        \n",
    "        # Extract multiple features from each window\n",
    "        window_features = [\n",
    "            np.mean(window),           # Mean amplitude\n",
    "            np.std(window),            # Standard deviation\n",
    "            np.var(window),            # Variance\n",
    "            np.max(window),            # Maximum value\n",
    "            np.min(window),            # Minimum value\n",
    "            np.percentile(window, 25), # 25th percentile\n",
    "            np.percentile(window, 75), # 75th percentile\n",
    "            np.mean(np.abs(window)),   # Mean absolute value\n",
    "        ]\n",
    "        \n",
    "        features.append(window_features)\n",
    "    \n",
    "    features_array = np.array(features)\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Created {len(features)} feature windows\")\n",
    "    print(f\"  ‚Ä¢ Feature dimension: {features_array.shape[1]}\")\n",
    "    print(f\"  ‚Ä¢ Window size: {feature_window} samples\")\n",
    "    print(f\"  ‚Ä¢ Overlap: {overlap*100:.0f}%\")\n",
    "    \n",
    "    return features_array\n",
    "\n",
    "def apply_gpu_hmm_analysis(signal, n_states=5, feature_window=512):\n",
    "    \"\"\"\n",
    "    Apply GPU-accelerated HMM analysis to EEG signal.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): Preprocessed EEG signal\n",
    "        n_states (int): Number of hidden states\n",
    "        feature_window (int): Window size for feature extraction\n",
    "    \n",
    "    Returns:\n",
    "        dict: HMM analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if signal is None:\n",
    "        print(\"‚ùå No signal provided for HMM analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"üöÄ STARTING GPU-ACCELERATED HMM ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Prepare features\n",
    "    features = prepare_features_for_hmm(signal, feature_window=feature_window)\n",
    "    \n",
    "    # Determine device\n",
    "    device_name = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "    print(f\"\\nüñ•Ô∏è  Using device: {device_name}\")\n",
    "    \n",
    "    with tf.device(device_name):\n",
    "        print(f\"\\nüéØ Training HMM with {n_states} states...\")\n",
    "        \n",
    "        # Initialize and train HMM\n",
    "        hmm_model = TensorFlowHMM(\n",
    "            n_components=n_states,\n",
    "            n_iter=100,\n",
    "            tol=1e-4,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        start_time = time.time()\n",
    "        hmm_model.fit(features)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚è±Ô∏è  Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        # Predict states\n",
    "        print(f\"\\nüîÆ Predicting hidden states...\")\n",
    "        predicted_states = hmm_model.predict(features)\n",
    "        state_probabilities = hmm_model.predict_proba(features)\n",
    "        \n",
    "        # Calculate state statistics\n",
    "        unique_states, state_counts = np.unique(predicted_states, return_counts=True)\n",
    "        state_percentages = (state_counts / len(predicted_states)) * 100\n",
    "        \n",
    "        print(f\"\\nüìä State Distribution:\")\n",
    "        for state, count, percentage in zip(unique_states, state_counts, state_percentages):\n",
    "            print(f\"  ‚Ä¢ State {state}: {count:,} windows ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            'model': hmm_model,\n",
    "            'predicted_states': predicted_states,\n",
    "            'state_probabilities': state_probabilities,\n",
    "            'features': features,\n",
    "            'training_time': training_time,\n",
    "            'n_states': n_states,\n",
    "            'feature_window': feature_window,\n",
    "            'state_distribution': dict(zip(unique_states, state_counts)),\n",
    "            'converged': hmm_model.converged_\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ HMM analysis completed successfully\")\n",
    "        return results\n",
    "\n",
    "# Apply HMM analysis if preprocessed signal is available\n",
    "if 'eeg_detrended' in locals() and eeg_detrended is not None:\n",
    "    \n",
    "    # HMM parameters\n",
    "    n_hidden_states = 5  # Wake, REM, N1, N2, N3 sleep stages\n",
    "    feature_window_size = int(sampling_freq)  # 1 second windows\n",
    "    \n",
    "    print(f\"üß† Applying Hidden Markov Model for sleep state classification...\")\n",
    "    print(f\"  ‚Ä¢ Number of states: {n_hidden_states}\")\n",
    "    print(f\"  ‚Ä¢ Feature window: {feature_window_size} samples ({feature_window_size/sampling_freq:.1f}s)\")\n",
    "    \n",
    "    # Run GPU-accelerated HMM\n",
    "    hmm_results = apply_gpu_hmm_analysis(\n",
    "        eeg_detrended, \n",
    "        n_states=n_hidden_states,\n",
    "        feature_window=feature_window_size\n",
    "    )\n",
    "    \n",
    "    if hmm_results:\n",
    "        print(f\"\\nüéØ HMM Analysis Summary:\")\n",
    "        print(f\"  ‚Ä¢ Training time: {hmm_results['training_time']:.2f} seconds\")\n",
    "        print(f\"  ‚Ä¢ Convergence: {'‚úÖ Yes' if hmm_results['converged'] else '‚ùå No'}\")\n",
    "        print(f\"  ‚Ä¢ Feature windows: {len(hmm_results['predicted_states']):,}\")\n",
    "        print(f\"  ‚Ä¢ Unique states found: {len(hmm_results['state_distribution'])}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No preprocessed EEG signal available for HMM analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb07538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CuPy is available for GPU acceleration\n"
     ]
    }
   ],
   "source": [
    "# Alternative: CPU-based HMM using scikit-learn's hmmlearn library\n",
    "def apply_cpu_hmm_comparison(signal, n_states=5, feature_window=512):\n",
    "    \"\"\"\n",
    "    Apply CPU-based HMM using hmmlearn for performance comparison.\n",
    "    \n",
    "    Args:\n",
    "        signal (np.array): Preprocessed EEG signal\n",
    "        n_states (int): Number of hidden states\n",
    "        feature_window (int): Window size for feature extraction\n",
    "    \n",
    "    Returns:\n",
    "        dict: CPU HMM results for comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  RUNNING CPU HMM FOR COMPARISON\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Prepare features (same as GPU version)\n",
    "    features = prepare_features_for_hmm(signal, feature_window=feature_window)\n",
    "    \n",
    "    print(f\"üéØ Training CPU-based HMM with {n_states} states...\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize CPU HMM model\n",
    "        cpu_model = hmm.GaussianHMM(\n",
    "            n_components=n_states,\n",
    "            covariance_type='diag',\n",
    "            n_iter=100,\n",
    "            tol=1e-4,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        start_time = time.time()\n",
    "        cpu_model.fit(features)\n",
    "        cpu_training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚è±Ô∏è  CPU training completed in {cpu_training_time:.2f} seconds\")\n",
    "        \n",
    "        # Predict states\n",
    "        cpu_states = cpu_model.predict(features)\n",
    "        cpu_probabilities = cpu_model.predict_proba(features)\n",
    "        \n",
    "        # Calculate state statistics\n",
    "        unique_states, state_counts = np.unique(cpu_states, return_counts=True)\n",
    "        \n",
    "        print(f\"üìä CPU HMM State Distribution:\")\n",
    "        for state, count in zip(unique_states, state_counts):\n",
    "            percentage = (count / len(cpu_states)) * 100\n",
    "            print(f\"  ‚Ä¢ State {state}: {count:,} windows ({percentage:.1f}%)\")\n",
    "        \n",
    "        cpu_results = {\n",
    "            'model': cpu_model,\n",
    "            'predicted_states': cpu_states,\n",
    "            'state_probabilities': cpu_probabilities,\n",
    "            'training_time': cpu_training_time,\n",
    "            'n_states': n_states,\n",
    "            'converged': True  # hmmlearn doesn't expose convergence status directly\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ CPU HMM analysis completed\")\n",
    "        return cpu_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå CPU HMM failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run CPU HMM for comparison if GPU results are available\n",
    "cpu_hmm_results = None\n",
    "if 'hmm_results' in locals() and hmm_results is not None:\n",
    "    \n",
    "    print(f\"\\nüîÑ Running CPU comparison...\")\n",
    "    cpu_hmm_results = apply_cpu_hmm_comparison(\n",
    "        eeg_detrended,\n",
    "        n_states=n_hidden_states,\n",
    "        feature_window=feature_window_size\n",
    "    )\n",
    "    \n",
    "    # Performance comparison\n",
    "    if cpu_hmm_results:\n",
    "        print(f\"\\n‚ö° PERFORMANCE COMPARISON\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        gpu_time = hmm_results['training_time']\n",
    "        cpu_time = cpu_hmm_results['training_time']\n",
    "        \n",
    "        print(f\"GPU Training Time: {gpu_time:.2f} seconds\")\n",
    "        print(f\"CPU Training Time: {cpu_time:.2f} seconds\")\n",
    "        \n",
    "        if gpu_time > 0:\n",
    "            speedup = cpu_time / gpu_time\n",
    "            print(f\"Speedup Factor: {speedup:.2f}x {'üöÄ' if speedup > 1 else 'üêå'}\")\n",
    "        \n",
    "        # Compare convergence\n",
    "        print(f\"GPU Convergence: {'‚úÖ' if hmm_results['converged'] else '‚ùå'}\")\n",
    "        print(f\"CPU Convergence: {'‚úÖ' if cpu_hmm_results['converged'] else '‚ùå'}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping CPU comparison - no GPU results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f26191",
   "metadata": {},
   "source": [
    "## üìä Step 6: Results Visualization and Analysis\n",
    "\n",
    "Comprehensive visualization of HMM results including state sequences, probabilities, and model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hmm_visualization(hmm_results, cpu_results=None, signal=None, sampling_freq=512):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of HMM results.\n",
    "    \n",
    "    Args:\n",
    "        hmm_results (dict): GPU HMM results\n",
    "        cpu_results (dict): CPU HMM results for comparison\n",
    "        signal (np.array): Original preprocessed signal\n",
    "        sampling_freq (float): Sampling frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hmm_results:\n",
    "        print(\"‚ùå No HMM results to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(\"üé® Creating HMM visualization...\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Hidden Markov Model Analysis - Sleep EEG Classification', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract results\n",
    "    gpu_states = hmm_results['predicted_states']\n",
    "    gpu_probs = hmm_results['state_probabilities']\n",
    "    \n",
    "    # Plot 1: Original signal with state overlay (top portion)\n",
    "    if signal is not None:\n",
    "        ax1 = axes[0, 0]\n",
    "        \n",
    "        # Show first 10,000 samples for visibility\n",
    "        signal_portion = signal[:10000]\n",
    "        time_axis = np.arange(len(signal_portion)) / sampling_freq\n",
    "        \n",
    "        ax1.plot(time_axis, signal_portion, 'b-', alpha=0.7, linewidth=0.5, label='EEG Signal')\n",
    "        ax1.set_xlabel('Time (seconds)')\n",
    "        ax1.set_ylabel('Amplitude (standardized)')\n",
    "        ax1.set_title('Preprocessed EEG Signal (First 10k samples)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "    \n",
    "    # Plot 2: State sequence\n",
    "    ax2 = axes[0, 1]\n",
    "    state_time = np.arange(len(gpu_states)) * (hmm_results['feature_window'] / sampling_freq)\n",
    "    ax2.plot(state_time, gpu_states, 'o-', markersize=2, linewidth=1, color='red')\n",
    "    ax2.set_xlabel('Time (seconds)')\n",
    "    ax2.set_ylabel('Hidden State')\n",
    "    ax2.set_title('Predicted Sleep States (GPU HMM)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yticks(range(hmm_results['n_states']))\n",
    "    \n",
    "    # Plot 3: State probabilities heatmap\n",
    "    ax3 = axes[1, 0]\n",
    "    im = ax3.imshow(gpu_probs[:500, :].T, aspect='auto', cmap='viridis', origin='lower')\n",
    "    ax3.set_xlabel('Time Windows (first 500)')\n",
    "    ax3.set_ylabel('Hidden State')\n",
    "    ax3.set_title('State Probabilities Heatmap')\n",
    "    plt.colorbar(im, ax=ax3, label='Probability')\n",
    "    \n",
    "    # Plot 4: State distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    state_dist = hmm_results['state_distribution']\n",
    "    states = list(state_dist.keys())\n",
    "    counts = list(state_dist.values())\n",
    "    \n",
    "    bars = ax4.bar(states, counts, color='lightblue', edgecolor='darkblue')\n",
    "    ax4.set_xlabel('Hidden State')\n",
    "    ax4.set_ylabel('Number of Windows')\n",
    "    ax4.set_title('State Distribution')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    total_windows = sum(counts)\n",
    "    for bar, count in zip(bars, counts):\n",
    "        percentage = (count / total_windows) * 100\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(counts),\n",
    "                f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 5: Comparison with CPU (if available)\n",
    "    ax5 = axes[2, 0]\n",
    "    if cpu_results:\n",
    "        cpu_states = cpu_results['predicted_states']\n",
    "        \n",
    "        # Compare first 1000 time points\n",
    "        comparison_length = min(1000, len(gpu_states), len(cpu_states))\n",
    "        time_comp = np.arange(comparison_length)\n",
    "        \n",
    "        ax5.plot(time_comp, gpu_states[:comparison_length], 'r-', alpha=0.7, label='GPU HMM', linewidth=2)\n",
    "        ax5.plot(time_comp, cpu_states[:comparison_length], 'b--', alpha=0.7, label='CPU HMM', linewidth=2)\n",
    "        ax5.set_xlabel('Time Windows')\n",
    "        ax5.set_ylabel('Predicted State')\n",
    "        ax5.set_title('GPU vs CPU State Predictions')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate agreement\n",
    "        agreement = np.mean(gpu_states[:comparison_length] == cpu_states[:comparison_length])\n",
    "        ax5.text(0.02, 0.98, f'Agreement: {agreement:.1%}', transform=ax5.transAxes, \n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    else:\n",
    "        ax5.text(0.5, 0.5, 'No CPU comparison\\navailable', ha='center', va='center', \n",
    "                transform=ax5.transAxes, fontsize=12)\n",
    "        ax5.set_title('CPU Comparison (Not Available)')\n",
    "    \n",
    "    # Plot 6: Performance metrics\n",
    "    ax6 = axes[2, 1]\n",
    "    \n",
    "    # Performance data\n",
    "    perf_labels = ['GPU Training', 'CPU Training'] if cpu_results else ['GPU Training']\n",
    "    perf_times = [hmm_results['training_time']]\n",
    "    if cpu_results:\n",
    "        perf_times.append(cpu_results['training_time'])\n",
    "    \n",
    "    colors = ['lightcoral', 'lightblue'] if cpu_results else ['lightcoral']\n",
    "    bars = ax6.bar(perf_labels, perf_times, color=colors)\n",
    "    ax6.set_ylabel('Training Time (seconds)')\n",
    "    ax6.set_title('Training Performance Comparison')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add time labels on bars\n",
    "    for bar, time_val in zip(bars, perf_times):\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(perf_times),\n",
    "                f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ HMM visualization completed\")\n",
    "\n",
    "# Create visualization if results are available\n",
    "if 'hmm_results' in locals() and hmm_results is not None:\n",
    "    create_hmm_visualization(\n",
    "        hmm_results, \n",
    "        cpu_results=cpu_hmm_results if 'cpu_hmm_results' in locals() else None,\n",
    "        signal=eeg_detrended if 'eeg_detrended' in locals() else None,\n",
    "        sampling_freq=sampling_freq if 'sampling_freq' in locals() else 512\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No HMM results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beefb06",
   "metadata": {},
   "source": [
    "## üìà Step 7: Clinical Analysis and State Interpretation\n",
    "\n",
    "Analyze HMM results in the context of sleep medicine and provide clinical insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sleep_architecture(hmm_results, sampling_freq=512):\n",
    "    \"\"\"\n",
    "    Analyze sleep architecture and provide clinical insights from HMM results.\n",
    "    \n",
    "    Args:\n",
    "        hmm_results (dict): HMM analysis results\n",
    "        sampling_freq (float): EEG sampling frequency\n",
    "    \n",
    "    Returns:\n",
    "        dict: Clinical analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hmm_results:\n",
    "        print(\"‚ùå No HMM results available for clinical analysis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üè• CLINICAL SLEEP ARCHITECTURE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    states = hmm_results['predicted_states']\n",
    "    probabilities = hmm_results['state_probabilities']\n",
    "    feature_window = hmm_results['feature_window']\n",
    "    \n",
    "    # Calculate temporal parameters\n",
    "    window_duration_sec = feature_window / sampling_freq\n",
    "    total_recording_time = len(states) * window_duration_sec\n",
    "    \n",
    "    print(f\"Recording Summary:\")\n",
    "    print(f\"  ‚Ä¢ Total recording time: {total_recording_time/3600:.2f} hours\")\n",
    "    print(f\"  ‚Ä¢ Analysis windows: {len(states):,}\")\n",
    "    print(f\"  ‚Ä¢ Window duration: {window_duration_sec:.1f} seconds\")\n",
    "    print(f\"  ‚Ä¢ Detected states: {len(set(states))}\")\n",
    "    \n",
    "    # State duration analysis\n",
    "    print(f\"\\nüìä Sleep State Analysis:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    state_durations = {}\n",
    "    current_state = states[0]\n",
    "    current_duration = 1\n",
    "    state_episodes = {state: [] for state in set(states)}\n",
    "    \n",
    "    # Analyze continuous episodes\n",
    "    for i in range(1, len(states)):\n",
    "        if states[i] == current_state:\n",
    "            current_duration += 1\n",
    "        else:\n",
    "            # End of episode\n",
    "            episode_time_min = (current_duration * window_duration_sec) / 60\n",
    "            state_episodes[current_state].append(episode_time_min)\n",
    "            \n",
    "            current_state = states[i]\n",
    "            current_duration = 1\n",
    "    \n",
    "    # Add final episode\n",
    "    final_episode_time = (current_duration * window_duration_sec) / 60\n",
    "    state_episodes[current_state].append(final_episode_time)\n",
    "    \n",
    "    # Calculate statistics for each state\n",
    "    clinical_analysis = {}\n",
    "    \n",
    "    for state in sorted(set(states)):\n",
    "        episodes = state_episodes[state]\n",
    "        total_time_min = sum(episodes)\n",
    "        percentage = (total_time_min / (total_recording_time / 60)) * 100\n",
    "        \n",
    "        if episodes:\n",
    "            avg_episode = np.mean(episodes)\n",
    "            median_episode = np.median(episodes)\n",
    "            max_episode = np.max(episodes)\n",
    "            n_episodes = len(episodes)\n",
    "        else:\n",
    "            avg_episode = median_episode = max_episode = n_episodes = 0\n",
    "        \n",
    "        clinical_analysis[state] = {\n",
    "            'total_time_min': total_time_min,\n",
    "            'percentage': percentage,\n",
    "            'n_episodes': n_episodes,\n",
    "            'avg_episode_min': avg_episode,\n",
    "            'median_episode_min': median_episode,\n",
    "            'max_episode_min': max_episode\n",
    "        }\n",
    "        \n",
    "        print(f\"State {state}:\")\n",
    "        print(f\"  ‚Ä¢ Total time: {total_time_min:.1f} min ({percentage:.1f}%)\")\n",
    "        print(f\"  ‚Ä¢ Episodes: {n_episodes}\")\n",
    "        print(f\"  ‚Ä¢ Avg episode: {avg_episode:.1f} min\")\n",
    "        print(f\"  ‚Ä¢ Max episode: {max_episode:.1f} min\")\n",
    "        print()\n",
    "    \n",
    "    # Sleep fragmentation analysis\n",
    "    print(f\"üîÑ Sleep Fragmentation Analysis:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Count state transitions\n",
    "    transitions = 0\n",
    "    for i in range(1, len(states)):\n",
    "        if states[i] != states[i-1]:\n",
    "            transitions += 1\n",
    "    \n",
    "    transitions_per_hour = transitions / (total_recording_time / 3600)\n",
    "    fragmentation_index = transitions / len(states)  # Normalized fragmentation\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Total transitions: {transitions:,}\")\n",
    "    print(f\"  ‚Ä¢ Transitions per hour: {transitions_per_hour:.1f}\")\n",
    "    print(f\"  ‚Ä¢ Fragmentation index: {fragmentation_index:.3f}\")\n",
    "    \n",
    "    # State stability analysis\n",
    "    stability_scores = {}\n",
    "    for state in set(states):\n",
    "        state_probs = probabilities[states == state, state]\n",
    "        if len(state_probs) > 0:\n",
    "            stability_scores[state] = np.mean(state_probs)\n",
    "        else:\n",
    "            stability_scores[state] = 0\n",
    "    \n",
    "    print(f\"\\nüéØ State Confidence Analysis:\")\n",
    "    print(\"-\" * 32)\n",
    "    for state in sorted(stability_scores.keys()):\n",
    "        confidence = stability_scores[state]\n",
    "        print(f\"  ‚Ä¢ State {state} confidence: {confidence:.3f}\")\n",
    "    \n",
    "    # Clinical interpretation\n",
    "    print(f\"\\nü©∫ Clinical Interpretation:\")\n",
    "    print(\"-\" * 28)\n",
    "    \n",
    "    # Identify potential sleep stages based on temporal patterns\n",
    "    state_interpretations = {}\n",
    "    \n",
    "    for state, analysis in clinical_analysis.items():\n",
    "        avg_episode = analysis['avg_episode_min']\n",
    "        percentage = analysis['percentage']\n",
    "        confidence = stability_scores[state]\n",
    "        \n",
    "        # Heuristic sleep stage assignment based on episode duration and prevalence\n",
    "        if avg_episode > 60 and percentage > 20:  # Long, prevalent episodes\n",
    "            interpretation = \"Potential Deep Sleep (N3/SWS)\"\n",
    "        elif avg_episode > 30 and percentage > 15:  # Medium-long episodes\n",
    "            interpretation = \"Potential Light Sleep (N2)\"\n",
    "        elif avg_episode > 10 and percentage > 10:  # Medium episodes\n",
    "            interpretation = \"Potential Stage 1 Sleep (N1)\"\n",
    "        elif avg_episode < 10 and percentage < 15:  # Short, sporadic episodes\n",
    "            interpretation = \"Potential REM or Transition\"\n",
    "        elif percentage > 25:  # High percentage regardless of episode length\n",
    "            interpretation = \"Potential Wake/Alert State\"\n",
    "        else:\n",
    "            interpretation = \"Unclassified Sleep State\"\n",
    "        \n",
    "        state_interpretations[state] = interpretation\n",
    "        print(f\"  ‚Ä¢ State {state}: {interpretation}\")\n",
    "        print(f\"    - Confidence: {confidence:.2f}, Duration: {avg_episode:.1f}min, Time: {percentage:.1f}%\")\n",
    "    \n",
    "    # Summary recommendations\n",
    "    print(f\"\\nüí° Analysis Summary & Recommendations:\")\n",
    "    print(\"-\" * 42)\n",
    "    \n",
    "    if transitions_per_hour > 30:\n",
    "        print(\"  ‚Ä¢ HIGH fragmentation detected - possible sleep disorder\")\n",
    "    elif transitions_per_hour > 15:\n",
    "        print(\"  ‚Ä¢ MODERATE fragmentation - monitor sleep quality\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ NORMAL fragmentation levels - stable sleep\")\n",
    "    \n",
    "    overall_confidence = np.mean(list(stability_scores.values()))\n",
    "    if overall_confidence > 0.8:\n",
    "        print(\"  ‚Ä¢ HIGH model confidence - reliable state classification\")\n",
    "    elif overall_confidence > 0.6:\n",
    "        print(\"  ‚Ä¢ MODERATE model confidence - reasonable classification\")\n",
    "    else:\n",
    "        print(\"  ‚Ä¢ LOW model confidence - consider parameter adjustment\")\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Overall model confidence: {overall_confidence:.3f}\")\n",
    "    print(f\"  ‚Ä¢ Recommended follow-up: {'Clinical review advised' if transitions_per_hour > 20 else 'Normal sleep architecture'}\")\n",
    "    \n",
    "    return {\n",
    "        'clinical_analysis': clinical_analysis,\n",
    "        'transitions_per_hour': transitions_per_hour,\n",
    "        'fragmentation_index': fragmentation_index,\n",
    "        'stability_scores': stability_scores,\n",
    "        'state_interpretations': state_interpretations,\n",
    "        'overall_confidence': overall_confidence\n",
    "    }\n",
    "\n",
    "# Perform clinical analysis if HMM results are available\n",
    "if 'hmm_results' in locals() and hmm_results is not None:\n",
    "    clinical_results = analyze_sleep_architecture(\n",
    "        hmm_results, \n",
    "        sampling_freq=sampling_freq if 'sampling_freq' in locals() else 512\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Clinical analysis completed successfully\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No HMM results available for clinical analysis\")\n",
    "\n",
    "print(f\"\\nüéØ HMM ANALYSIS PIPELINE COMPLETED\")\n",
    "print(\"=\" * 40)\n",
    "print(\"‚úÖ GPU-accelerated Hidden Markov Model analysis finished\")\n",
    "print(\"üìä Sleep state classification and clinical insights generated\")\n",
    "print(\"üè• Results ready for clinical interpretation and further analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
