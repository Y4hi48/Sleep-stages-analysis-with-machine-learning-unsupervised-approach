{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1680399d",
   "metadata": {},
   "source": [
    "# BiLSTM + Time Series Transformer for Self-Supervised Signal Reconstruction and Clustering\n",
    "\n",
    "This notebook implements a sophisticated deep learning pipeline that combines **BiLSTM** and **Time Series Transformer** architectures for self-supervised signal reconstruction and clustering of EEG sleep stage data.\n",
    "\n",
    "## Key Features:\n",
    "- **3-second time windows** for precise temporal analysis\n",
    "- **BiLSTM + Transformer hybrid model** for capturing both sequential and attention-based patterns\n",
    "- **Self-supervised signal reconstruction** without requiring labeled data\n",
    "- **DMD feature extraction** for capturing dynamic mode information\n",
    "- **HDBSCAN clustering** for discovering hidden states in the embedding space\n",
    "- **Multi-channel support** for EEG, EOG, and other physiological signals\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. Load and preprocess EEG/EDF files\n",
    "2. Extract DMD features from 3-second windows\n",
    "3. Create sequences for model input\n",
    "4. Train BiLSTM + Transformer hybrid model\n",
    "5. Extract embeddings from trained model\n",
    "6. Apply HDBSCAN clustering\n",
    "7. Visualize results and save outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f4a74",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for the pipeline including deep learning frameworks, signal processing, and clustering tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ae5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Signal processing and EEG\n",
    "import mne\n",
    "from scipy.signal import butter, filtfilt\n",
    "from scipy.stats import mode\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "# DMD for feature extraction\n",
    "from pydmd import DMD, EDMD\n",
    "\n",
    "# Machine learning and clustering\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import hdbscan\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"Warning: CUDA not available. Using CPU.\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114bbebf",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Preprocessing\n",
    "\n",
    "Load EEG/EDF files, normalize signals, apply bandpass filtering, and remove outliers using a sliding window approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ead28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(signal, sfreq, low=0.5, high=40, order=4):\n",
    "    \"\"\"Apply bandpass filter to EEG signal.\"\"\"\n",
    "    nyq = 0.5 * sfreq\n",
    "    lowcut = low / nyq\n",
    "    highcut = high / nyq\n",
    "    b, a = butter(order, [lowcut, highcut], btype='band')\n",
    "    filtered = filtfilt(b, a, signal)\n",
    "    return filtered\n",
    "\n",
    "def load_and_preprocess_edf(filepath, num_channels=4, window_sec=30, std_factor=3):\n",
    "    \"\"\"Load and preprocess multiple channels from an EDF file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the EDF file\n",
    "    num_channels : int\n",
    "        Number of channels to load (default: 4)\n",
    "    window_sec : int\n",
    "        Window size in seconds for outlier removal\n",
    "    std_factor : int\n",
    "        Standard deviation multiplier for outlier detection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    signals : list of numpy.ndarray\n",
    "        List of preprocessed signals\n",
    "    sfreqs : list of int\n",
    "        List of sampling frequencies for each signal\n",
    "    channel_names : list of str\n",
    "        List of channel names\n",
    "    \"\"\"\n",
    "    print(f\"Loading EDF file: {filepath}\")\n",
    "    raw = mne.io.read_raw_edf(filepath, preload=True, verbose=False)\n",
    "    \n",
    "    # Get channel names and make sure we don't request more than available\n",
    "    all_channels = raw.ch_names\n",
    "    num_channels = min(num_channels, len(all_channels))\n",
    "    channels_to_use = all_channels[:num_channels]\n",
    "    \n",
    "    signals = []\n",
    "    sfreqs = []\n",
    "    \n",
    "    print(f\"Processing {num_channels} channels: {channels_to_use}\")\n",
    "    \n",
    "    # Process each channel\n",
    "    for i, channel in enumerate(channels_to_use):\n",
    "        print(f\"  Processing channel {i+1}/{num_channels}: {channel}\")\n",
    "        \n",
    "        # Extract signal and sampling frequency for this channel\n",
    "        signal = raw.get_data(picks=channel)[0]\n",
    "        sfreq = int(raw.info['sfreq'])\n",
    "        \n",
    "        # Bandpass filter 0.5-40Hz (for sleep stage analysis)\n",
    "        signal = bandpass_filter(signal, sfreq, low=0.5, high=40)\n",
    "        \n",
    "        # Normalize to [-1, 1]\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        signal_norm = scaler.fit_transform(signal.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Remove outliers in sliding windows\n",
    "        win_samples = window_sec * sfreq\n",
    "        cleaned = signal_norm.copy()\n",
    "        \n",
    "        for start in range(0, len(signal_norm), win_samples):\n",
    "            end = min(start + win_samples, len(signal_norm))\n",
    "            window = signal_norm[start:end]\n",
    "            mean = np.mean(window)\n",
    "            std = np.std(window)\n",
    "            mask = np.abs(window - mean) < std_factor * std\n",
    "            window_cleaned = np.where(mask, window, mean)\n",
    "            cleaned[start:end] = window_cleaned\n",
    "            \n",
    "        signals.append(cleaned)\n",
    "        sfreqs.append(sfreq)\n",
    "        \n",
    "        print(f\"    Signal length: {len(cleaned)} samples ({len(cleaned)/sfreq:.1f} seconds)\")\n",
    "    \n",
    "    print(f\"✓ Successfully loaded {len(signals)} channels\")\n",
    "    return signals, sfreqs, channels_to_use\n",
    "\n",
    "# EDF file list for processing\n",
    "edf_files = [\n",
    "    \"raw data/SC4001E0-PSG.edf\",\n",
    "    \"raw data/SC4002E0-PSG.edf\", \n",
    "    \"raw data/SC4011E0-PSG.edf\",\n",
    "    \"raw data/SC4012E0-PSG.edf\",\n",
    "    \"raw data/SC4021E0-PSG.edf\"\n",
    "]\n",
    "\n",
    "print(f\"Will process {len(edf_files)} EDF files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd83457",
   "metadata": {},
   "source": [
    "## Section 3: Feature Extraction with DMD on 3-second Windows\n",
    "\n",
    "Extract DMD features (magnitude and phase) from each 3-second window of the preprocessed signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea7b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dmd_features_3s(signal, sfreq, window_sec=3, step_sec=1, n_modes=8):\n",
    "    \"\"\"Extract DMD features from 3-second windows with 1-second steps.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    signal : numpy.ndarray\n",
    "        Input signal array\n",
    "    sfreq : float\n",
    "        Sampling frequency in Hz\n",
    "    window_sec : int\n",
    "        Window size in seconds (fixed at 3 for this pipeline)\n",
    "    step_sec : int\n",
    "        Step size in seconds (1 for high temporal resolution)\n",
    "    n_modes : int\n",
    "        Number of DMD modes to extract\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    features : numpy.ndarray\n",
    "        Extracted DMD features (n_windows x n_features)\n",
    "    time_stamps : numpy.ndarray\n",
    "        Time stamps for each window\n",
    "    \"\"\"\n",
    "    win_samples = int(window_sec * sfreq)\n",
    "    step_samples = int(step_sec * sfreq)\n",
    "    \n",
    "    # Calculate number of windows\n",
    "    n_windows = (len(signal) - win_samples) // step_samples + 1\n",
    "    \n",
    "    features = []\n",
    "    time_stamps = []\n",
    "    \n",
    "    print(f\"Extracting DMD features from {n_windows} windows (3s each, 1s step)...\")\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for start in tqdm(range(0, len(signal) - win_samples + 1, step_samples), \n",
    "                      desc=\"DMD feature extraction\"):\n",
    "        \n",
    "        window = signal[start:start + win_samples]\n",
    "        \n",
    "        # Create DMD instance\n",
    "        dmd = DMD(svd_rank=n_modes)\n",
    "        \n",
    "        # Fit DMD model to the window data\n",
    "        # DMD expects data in format (n_features, n_samples)\n",
    "        dmd.fit(window.reshape(1, -1))\n",
    "        \n",
    "        # Extract magnitude and phase features from the amplitudes\n",
    "        if len(dmd.amplitudes) >= n_modes:\n",
    "            amplitudes = dmd.amplitudes[:n_modes]\n",
    "        else:\n",
    "            # Pad with zeros if fewer modes than expected\n",
    "            amplitudes = np.pad(dmd.amplitudes, (0, n_modes - len(dmd.amplitudes)), \n",
    "                              mode='constant', constant_values=0)\n",
    "        \n",
    "        feat_mag = np.abs(amplitudes)  # Magnitude features\n",
    "        feat_phase = np.angle(amplitudes)  # Phase features\n",
    "        \n",
    "        # Also extract eigenvalues (frequency and growth rate information)\n",
    "        if len(dmd.eigs) >= n_modes:\n",
    "            eigenvalues = dmd.eigs[:n_modes]\n",
    "        else:\n",
    "            eigenvalues = np.pad(dmd.eigs, (0, n_modes - len(dmd.eigs)), \n",
    "                               mode='constant', constant_values=0)\n",
    "        \n",
    "        feat_freq = np.abs(eigenvalues)  # Frequency-like features\n",
    "        feat_growth = np.real(eigenvalues)  # Growth rate features\n",
    "        \n",
    "        # Combine all features\n",
    "        combined_features = np.concatenate([feat_mag, feat_phase, feat_freq, feat_growth])\n",
    "        \n",
    "        features.append(combined_features)\n",
    "        time_stamps.append(start / sfreq)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    time_stamps = np.array(time_stamps)\n",
    "    \n",
    "    print(f\"✓ Extracted {features.shape[0]} feature vectors with {features.shape[1]} features each\")\n",
    "    print(f\"  Feature components: {n_modes} magnitude + {n_modes} phase + {n_modes} frequency + {n_modes} growth\")\n",
    "    \n",
    "    return features, time_stamps\n",
    "\n",
    "def process_all_files_dmd(edf_files, save_dir='features_3s'):\n",
    "    \"\"\"Process all EDF files and extract DMD features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    edf_files : list\n",
    "        List of EDF file paths\n",
    "    save_dir : str\n",
    "        Directory to save features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_dict : dict\n",
    "        Dictionary containing features for each file and channel\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    feature_dict = {}\n",
    "    \n",
    "    for file_path in edf_files:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {file_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Load and preprocess\n",
    "        signals, sfreqs, channel_names = load_and_preprocess_edf(file_path, num_channels=4)\n",
    "        \n",
    "        file_basename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        feature_dict[file_basename] = {}\n",
    "        \n",
    "        # Process each channel\n",
    "        for i, (signal, sfreq, channel) in enumerate(zip(signals, sfreqs, channel_names)):\n",
    "            print(f\"\\nChannel {i+1}/{len(signals)}: {channel}\")\n",
    "            \n",
    "            # Extract DMD features\n",
    "            features, time_stamps = extract_dmd_features_3s(signal, sfreq)\n",
    "            \n",
    "            # Store in dictionary\n",
    "            feature_dict[file_basename][channel] = {\n",
    "                'features': features,\n",
    "                'time_stamps': time_stamps,\n",
    "                'sfreq': sfreq\n",
    "            }\n",
    "            \n",
    "            # Save features\n",
    "            save_path = os.path.join(save_dir, f\"{file_basename}_{channel}_features_3s.npy\")\n",
    "            np.save(save_path, features)\n",
    "            \n",
    "            # Save time stamps\n",
    "            time_path = os.path.join(save_dir, f\"{file_basename}_{channel}_timestamps_3s.npy\")\n",
    "            np.save(time_path, time_stamps)\n",
    "            \n",
    "            print(f\"  ✓ Saved features: {save_path}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ All files processed successfully!\")\n",
    "    print(f\"Total files: {len(feature_dict)}\")\n",
    "    print(f\"Total channels per file: {len(next(iter(feature_dict.values())))}\")\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca819df",
   "metadata": {},
   "source": [
    "## Section 4: Sequence Creation for Model Input\n",
    "\n",
    "Create overlapping sequences of DMD features for each channel to be used as input for the hybrid model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a209670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences_from_features(feature_dict, seq_length=20, overlap=0.5):\n",
    "    \"\"\"Create overlapping sequences from feature dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_dict : dict\n",
    "        Dictionary of features for each file and channel\n",
    "    seq_length : int\n",
    "        Length of sequences to create (number of 3-second windows)\n",
    "    overlap : float\n",
    "        Overlap between consecutive sequences (0.0-1.0)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    sequences : dict\n",
    "        Dictionary of sequences for each file and channel\n",
    "    \"\"\"\n",
    "    step = int(seq_length * (1 - overlap))\n",
    "    step = max(1, step)  # Ensure step is at least 1\n",
    "    \n",
    "    sequences = {}\n",
    "    \n",
    "    for file_name, channels in feature_dict.items():\n",
    "        sequences[file_name] = {}\n",
    "        \n",
    "        for channel, data in channels.items():\n",
    "            features = data['features']\n",
    "            time_stamps = data['time_stamps']\n",
    "            \n",
    "            channel_sequences = []\n",
    "            seq_timestamps = []\n",
    "            \n",
    "            # Create sequences\n",
    "            for i in range(0, len(features) - seq_length + 1, step):\n",
    "                seq = features[i:i + seq_length]\n",
    "                seq_time = time_stamps[i:i + seq_length]\n",
    "                \n",
    "                channel_sequences.append(seq)\n",
    "                seq_timestamps.append(seq_time)\n",
    "            \n",
    "            if channel_sequences:\n",
    "                sequences[file_name][channel] = {\n",
    "                    'sequences': np.array(channel_sequences),\n",
    "                    'timestamps': np.array(seq_timestamps)\n",
    "                }\n",
    "                \n",
    "                print(f\"{file_name} - {channel}: {len(channel_sequences)} sequences \"\n",
    "                      f\"(shape: {np.array(channel_sequences).shape})\")\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "class MultiChannelSequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-channel sequence data.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences_dict):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        sequences_dict : dict\n",
    "            Dictionary of sequences for each file and channel\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.file_names = []\n",
    "        self.timestamps = []\n",
    "        \n",
    "        # Flatten all sequences from all files and channels\n",
    "        for file_name, channels in sequences_dict.items():\n",
    "            # Get the first channel to determine sequence count\n",
    "            first_channel = next(iter(channels.keys()))\n",
    "            n_sequences = len(channels[first_channel]['sequences'])\n",
    "            \n",
    "            for seq_idx in range(n_sequences):\n",
    "                # Create a sample with all channels\n",
    "                sample = {}\n",
    "                timestamp = None\n",
    "                \n",
    "                for channel, data in channels.items():\n",
    "                    sample[channel] = torch.tensor(data['sequences'][seq_idx], dtype=torch.float32)\n",
    "                    if timestamp is None:\n",
    "                        timestamp = data['timestamps'][seq_idx]\n",
    "                \n",
    "                self.data.append(sample)\n",
    "                self.file_names.append(file_name)\n",
    "                self.timestamps.append(timestamp)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.file_names[idx], self.timestamps[idx]\n",
    "\n",
    "def collate_multichannel_sequences(batch):\n",
    "    \"\"\"Custom collate function for multi-channel sequences.\"\"\"\n",
    "    samples, file_names, timestamps = zip(*batch)\n",
    "    \n",
    "    # Get channel names from first sample\n",
    "    channels = list(samples[0].keys())\n",
    "    \n",
    "    # Stack sequences for each channel\n",
    "    batched_data = {}\n",
    "    for channel in channels:\n",
    "        channel_data = [sample[channel] for sample in samples]\n",
    "        batched_data[channel] = torch.stack(channel_data)\n",
    "    \n",
    "    return batched_data, file_names, timestamps\n",
    "\n",
    "print(\"✓ Sequence creation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e91d6d",
   "metadata": {},
   "source": [
    "## Section 5: Define BiLSTM + Time Series Transformer Hybrid Model\n",
    "\n",
    "Implement a PyTorch model that combines BiLSTM layers with a transformer encoder, supporting multi-channel input and positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e936fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTransformerHybrid(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid model combining BiLSTM and Transformer for self-supervised signal reconstruction.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Input projection for each channel\n",
    "    2. BiLSTM layers for sequential modeling\n",
    "    3. Transformer encoder for attention-based modeling\n",
    "    4. Reconstruction heads for each channel\n",
    "    5. Embedding extraction for clustering\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dims, d_model=256, lstm_hidden=128, lstm_layers=2, \n",
    "                 nhead=8, num_transformer_layers=4, embedding_dim=64, \n",
    "                 dropout=0.1, seq_length=20):\n",
    "        super(BiLSTMTransformerHybrid, self).__init__()\n",
    "        \n",
    "        self.input_dims = input_dims\n",
    "        self.d_model = d_model\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Input projections for each channel\n",
    "        self.input_projs = nn.ModuleDict({\n",
    "            channel: nn.Sequential(\n",
    "                nn.Linear(dim, d_model // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model // 2, d_model)\n",
    "            ) for channel, dim in input_dims.items()\n",
    "        })\n",
    "        \n",
    "        # BiLSTM layers for each channel\n",
    "        self.bilstm_layers = nn.ModuleDict({\n",
    "            channel: nn.LSTM(\n",
    "                input_size=d_model,\n",
    "                hidden_size=lstm_hidden,\n",
    "                num_layers=lstm_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if lstm_layers > 1 else 0,\n",
    "                bidirectional=True\n",
    "            ) for channel in input_dims.keys()\n",
    "        })\n",
    "        \n",
    "        # Project BiLSTM output to d_model\n",
    "        self.lstm_proj = nn.ModuleDict({\n",
    "            channel: nn.Linear(lstm_hidden * 2, d_model)  # *2 for bidirectional\n",
    "            for channel in input_dims.keys()\n",
    "        })\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, seq_length)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation='relu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_transformer_layers)\n",
    "        \n",
    "        # Reconstruction heads for each channel\n",
    "        self.reconstruction_heads = nn.ModuleDict({\n",
    "            channel: nn.Sequential(\n",
    "                nn.Linear(d_model, d_model // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model // 2, dim)\n",
    "            ) for channel, dim in input_dims.items()\n",
    "        })\n",
    "        \n",
    "        # Embedding projection for clustering\n",
    "        self.embedding_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, embedding_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, inputs, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the hybrid model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        inputs : dict\n",
    "            Dictionary of channel inputs, each with shape [batch_size, seq_length, feature_dim]\n",
    "        return_embeddings : bool\n",
    "            If True, return embeddings for clustering\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        reconstructed : dict\n",
    "            Reconstructed signals for each channel\n",
    "        embeddings : dict (optional)\n",
    "            Embeddings for clustering if return_embeddings=True\n",
    "        \"\"\"\n",
    "        batch_size = next(iter(inputs.values())).shape[0]\n",
    "        \n",
    "        # Process each channel\n",
    "        lstm_outputs = {}\n",
    "        transformer_outputs = {}\n",
    "        \n",
    "        for channel, x in inputs.items():\n",
    "            # Input projection\n",
    "            x = self.input_projs[channel](x)  # [batch, seq_len, d_model]\n",
    "            \n",
    "            # BiLSTM processing\n",
    "            lstm_out, _ = self.bilstm_layers[channel](x)  # [batch, seq_len, lstm_hidden*2]\n",
    "            lstm_out = self.lstm_proj[channel](lstm_out)  # [batch, seq_len, d_model]\n",
    "            \n",
    "            # Add residual connection\n",
    "            x = x + lstm_out\n",
    "            x = self.layer_norm(x)\n",
    "            \n",
    "            lstm_outputs[channel] = x\n",
    "        \n",
    "        # Combine channels for transformer processing\n",
    "        # Simple approach: concatenate along feature dimension then project back\n",
    "        combined = torch.cat(list(lstm_outputs.values()), dim=-1)  # [batch, seq_len, d_model*n_channels]\n",
    "        \n",
    "        # Project back to d_model\n",
    "        combined_proj = nn.Linear(combined.shape[-1], self.d_model).to(combined.device)\n",
    "        combined = combined_proj(combined)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        # Transformer processing\n",
    "        # Permute for transformer: [seq_len, batch, d_model]\n",
    "        combined = combined.permute(1, 0, 2)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        combined = self.pos_encoder(combined)\n",
    "        \n",
    "        # Apply transformer\n",
    "        transformer_out = self.transformer(combined)  # [seq_len, batch, d_model]\n",
    "        \n",
    "        # Permute back: [batch, seq_len, d_model]\n",
    "        transformer_out = transformer_out.permute(1, 0, 2)\n",
    "        \n",
    "        # Reconstruction for each channel\n",
    "        reconstructed = {}\n",
    "        for channel in inputs.keys():\n",
    "            reconstructed[channel] = self.reconstruction_heads[channel](transformer_out)\n",
    "        \n",
    "        # Extract embeddings if requested\n",
    "        if return_embeddings:\n",
    "            # Use mean pooling over sequence length\n",
    "            pooled = transformer_out.mean(dim=1)  # [batch, d_model]\n",
    "            embeddings = self.embedding_proj(pooled)  # [batch, embedding_dim]\n",
    "            \n",
    "            # Return embeddings for all channels (same embedding represents the combined signal)\n",
    "            embedding_dict = {channel: embeddings for channel in inputs.keys()}\n",
    "            return reconstructed, embedding_dict\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "print(\"✓ BiLSTM + Transformer hybrid model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8c9c01",
   "metadata": {},
   "source": [
    "## Section 6: Model Training for Self-Supervised Signal Reconstruction\n",
    "\n",
    "Train the hybrid model to reconstruct the input signals in a self-supervised manner using MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe91b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_model(sequences_dict, input_dims, epochs=50, batch_size=32, \n",
    "                      learning_rate=1e-4, weight_decay=1e-5, focus_channel=None):\n",
    "    \"\"\"\n",
    "    Train the BiLSTM + Transformer hybrid model for self-supervised reconstruction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sequences_dict : dict\n",
    "        Dictionary of sequences for each file and channel\n",
    "    input_dims : dict\n",
    "        Dictionary of input dimensions for each channel\n",
    "    epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    learning_rate : float\n",
    "        Learning rate for optimizer\n",
    "    weight_decay : float\n",
    "        Weight decay for regularization\n",
    "    focus_channel : str or None\n",
    "        Channel to focus on for reconstruction loss (if None, use all channels)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model : BiLSTMTransformerHybrid\n",
    "        Trained model\n",
    "    training_history : dict\n",
    "        Training loss history\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MultiChannelSequenceDataset(sequences_dict)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_multichannel_sequences,\n",
    "        num_workers=0  # Set to 0 for Windows compatibility\n",
    "    )\\n    \n",
    "    print(f\"Training dataset size: {len(dataset)} sequences\")\n",
    "    print(f\"Batch size: {batch_size}, Batches per epoch: {len(dataloader)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = BiLSTMTransformerHybrid(input_dims).to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model parameters: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training history\n",
    "    training_history = {\n",
    "        'epoch_losses': [],\n",
    "        'channel_losses': {channel: [] for channel in input_dims.keys()},\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"\\\\nStarting training for {epochs} epochs...\")\n",
    "    print(f\"Focus channel: {focus_channel if focus_channel else 'All channels'}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        channel_losses = {channel: 0.0 for channel in input_dims.keys()}\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        \n",
    "        for batch_idx, (batch_data, file_names, timestamps) in enumerate(pbar):\n",
    "            # Move data to device\n",
    "            batch_data = {channel: data.to(device) for channel, data in batch_data.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            reconstructed = model(batch_data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            if focus_channel and focus_channel in batch_data:\n",
    "                # Focus on specific channel\n",
    "                loss = criterion(reconstructed[focus_channel], batch_data[focus_channel])\n",
    "                total_loss = loss\n",
    "                channel_losses[focus_channel] += loss.item()\n",
    "            else:\n",
    "                # Use all channels\n",
    "                for channel in batch_data.keys():\n",
    "                    loss = criterion(reconstructed[channel], batch_data[channel])\n",
    "                    total_loss += loss\n",
    "                    channel_losses[channel] += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': total_loss.item():.6f})\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        avg_channel_losses = {channel: loss / len(dataloader) for channel, loss in channel_losses.items()}\n",
    "        \n",
    "        # Store history\n",
    "        training_history['epoch_losses'].append(avg_epoch_loss)\n",
    "        training_history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
    "        for channel, loss in avg_channel_losses.items():\n",
    "            training_history['channel_losses'][channel].append(loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{epochs}: Loss = {avg_epoch_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.2e}')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': avg_epoch_loss,\n",
    "                'input_dims': input_dims,\n",
    "                'training_history': training_history\n",
    "            }\n",
    "            torch.save(checkpoint, f'bilstm_transformer_checkpoint_epoch_{epoch+1}.pt')\n",
    "            print(f\"  ✓ Checkpoint saved\")\n",
    "    \n",
    "    print(f\"\\\\n✓ Training completed!\")\n",
    "    return model, training_history\n",
    "\n",
    "def plot_training_history(training_history):\n",
    "    \"\"\"Plot training history.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot overall loss\n",
    "    axes[0].plot(training_history['epoch_losses'], 'b-', linewidth=2, label='Total Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training Loss Over Time')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Plot learning rate\n",
    "    axes[1].plot(training_history['learning_rates'], 'r-', linewidth=2, label='Learning Rate')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Learning Rate')\n",
    "    axes[1].set_title('Learning Rate Schedule')\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot channel-specific losses\n",
    "    if len(training_history['channel_losses']) > 1:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for channel, losses in training_history['channel_losses'].items():\n",
    "            plt.plot(losses, linewidth=2, label=f'{channel} Loss')\n",
    "        \n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Channel-Specific Training Losses')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80387a",
   "metadata": {},
   "source": [
    "## Section 7: Extract Embeddings from Trained Model\n",
    "\n",
    "Obtain embeddings from the model's hidden states for each 3-second window for downstream clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ebca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_model(model, sequences_dict, batch_size=64):\n",
    "    \"\"\"\n",
    "    Extract embeddings from the trained hybrid model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : BiLSTMTransformerHybrid\n",
    "        Trained model\n",
    "    sequences_dict : dict\n",
    "        Dictionary of sequences for each file and channel\n",
    "    batch_size : int\n",
    "        Batch size for inference\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    embeddings : numpy.ndarray\n",
    "        Extracted embeddings for all sequences\n",
    "    metadata : list\n",
    "        Metadata for each embedding (file_name, timestamp)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MultiChannelSequenceDataset(sequences_dict)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,  # Important: don't shuffle for consistent ordering\n",
    "        collate_fn=collate_multichannel_sequences,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "    \n",
    "    print(f\"Extracting embeddings from {len(dataset)} sequences...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, file_names, timestamps in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            # Move data to device\n",
    "            batch_data = {channel: data.to(device) for channel, data in batch_data.items()}\n",
    "            \n",
    "            # Get embeddings\n",
    "            _, batch_embeddings = model(batch_data, return_embeddings=True)\n",
    "            \n",
    "            # We use the first channel's embeddings (they're all the same for combined signals)\n",
    "            first_channel = next(iter(batch_embeddings.keys()))\n",
    "            batch_emb = batch_embeddings[first_channel].cpu().numpy()\n",
    "            \n",
    "            embeddings.append(batch_emb)\n",
    "            \n",
    "            # Store metadata\n",
    "            for i in range(len(file_names)):\n",
    "                metadata.append({\n",
    "                    'file_name': file_names[i],\n",
    "                    'timestamp_start': timestamps[i][0],\n",
    "                    'timestamp_end': timestamps[i][-1],\n",
    "                    'sequence_length': len(timestamps[i])\n",
    "                })\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    \n",
    "    print(f\"✓ Extracted {embeddings.shape[0]} embeddings with {embeddings.shape[1]} dimensions\")\n",
    "    \n",
    "    return embeddings, metadata\n",
    "\n",
    "def save_embeddings(embeddings, metadata, save_path='embeddings_3s'):\n",
    "    \"\"\"Save embeddings and metadata to disk.\"\"\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save embeddings\n",
    "    emb_file = os.path.join(save_path, f\"embeddings_{timestamp}.npy\")\n",
    "    np.save(emb_file, embeddings)\n",
    "    \n",
    "    # Save metadata\n",
    "    meta_file = os.path.join(save_path, f\"metadata_{timestamp}.json\")\n",
    "    with open(meta_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Save as CSV for compatibility\n",
    "    csv_file = os.path.join(save_path, f\"embeddings_{timestamp}.csv\")\n",
    "    emb_df = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])\n",
    "    \n",
    "    # Add metadata columns\n",
    "    for key in metadata[0].keys():\n",
    "        emb_df[key] = [m[key] for m in metadata]\n",
    "    \n",
    "    emb_df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    print(f\"✓ Embeddings saved:\")\n",
    "    print(f\"  - {emb_file}\")\n",
    "    print(f\"  - {meta_file}\")\n",
    "    print(f\"  - {csv_file}\")\n",
    "    \n",
    "    return emb_file, meta_file, csv_file\n",
    "\n",
    "print(\"✓ Embedding extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02c7e5",
   "metadata": {},
   "source": [
    "## Section 8: Cluster Embeddings Using HDBSCAN\n",
    "\n",
    "Apply HDBSCAN to the extracted embeddings to discover clusters representing hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced837f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings_hdbscan(embeddings, min_cluster_size=50, min_samples=10, \n",
    "                              metric='euclidean', cluster_selection_epsilon=0.0):\n",
    "    \"\"\"\n",
    "    Perform HDBSCAN clustering on embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Embeddings to cluster\n",
    "    min_cluster_size : int\n",
    "        Minimum size of clusters\n",
    "    min_samples : int\n",
    "        Minimum number of samples in a neighborhood for a point to be considered a core point\n",
    "    metric : str\n",
    "        Distance metric for clustering\n",
    "    cluster_selection_epsilon : float\n",
    "        Distance threshold for cluster selection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    cluster_labels : numpy.ndarray\n",
    "        Cluster labels for each embedding\n",
    "    clusterer : hdbscan.HDBSCAN\n",
    "        Fitted HDBSCAN object\n",
    "    cluster_stats : dict\n",
    "        Statistics about the clustering\n",
    "    \"\"\"\n",
    "    print(f\"Clustering {embeddings.shape[0]} embeddings with {embeddings.shape[1]} dimensions...\")\n",
    "    print(f\"HDBSCAN parameters: min_cluster_size={min_cluster_size}, min_samples={min_samples}\")\n",
    "    \n",
    "    # Standardize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    # Apply HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric=metric,\n",
    "        cluster_selection_epsilon=cluster_selection_epsilon\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clusterer.fit_predict(scaled_embeddings)\n",
    "    \n",
    "    # Calculate cluster statistics\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)  # Exclude noise\n",
    "    n_noise = np.sum(cluster_labels == -1)\n",
    "    \n",
    "    cluster_stats = {\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'n_total': len(cluster_labels),\n",
    "        'noise_ratio': n_noise / len(cluster_labels),\n",
    "        'cluster_sizes': dict(zip(unique_labels, counts)),\n",
    "        'silhouette_score': silhouette_score(scaled_embeddings, cluster_labels) if n_clusters > 1 else -1\n",
    "    }\n",
    "    \n",
    "    print(f\"✓ Clustering completed:\")\n",
    "    print(f\"  - Number of clusters: {n_clusters}\")\n",
    "    print(f\"  - Noise points: {n_noise} ({cluster_stats['noise_ratio']:.2%})\")\n",
    "    print(f\"  - Silhouette score: {cluster_stats['silhouette_score']:.3f}\")\n",
    "    \n",
    "    # Print cluster sizes\n",
    "    print(f\"  - Cluster sizes:\")\n",
    "    for label, size in sorted(cluster_stats['cluster_sizes'].items()):\n",
    "        if label == -1:\n",
    "            print(f\"    Noise: {size}\")\n",
    "        else:\n",
    "            print(f\"    Cluster {label}: {size}\")\n",
    "    \n",
    "    return cluster_labels, clusterer, cluster_stats\n",
    "\n",
    "def optimize_hdbscan_parameters(embeddings, min_cluster_sizes=None, min_samples_list=None):\n",
    "    \"\"\"\n",
    "    Optimize HDBSCAN parameters using silhouette score.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Embeddings to cluster\n",
    "    min_cluster_sizes : list\n",
    "        List of min_cluster_size values to try\n",
    "    min_samples_list : list\n",
    "        List of min_samples values to try\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    best_params : dict\n",
    "        Best parameters found\n",
    "    results : list\n",
    "        List of all results\n",
    "    \"\"\"\n",
    "    if min_cluster_sizes is None:\n",
    "        min_cluster_sizes = [20, 30, 50, 75, 100]\n",
    "    \n",
    "    if min_samples_list is None:\n",
    "        min_samples_list = [5, 10, 15, 20]\n",
    "    \n",
    "    print(\"Optimizing HDBSCAN parameters...\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaled_embeddings = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    results = []\n",
    "    best_score = -1\n",
    "    best_params = None\n",
    "    \n",
    "    for min_cluster_size in min_cluster_sizes:\n",
    "        for min_samples in min_samples_list:\n",
    "            print(f\"  Testing: min_cluster_size={min_cluster_size}, min_samples={min_samples}\")\n",
    "            \n",
    "            try:\n",
    "                clusterer = hdbscan.HDBSCAN(\n",
    "                    min_cluster_size=min_cluster_size,\n",
    "                    min_samples=min_samples,\n",
    "                    metric='euclidean'\n",
    "                )\n",
    "                \n",
    "                labels = clusterer.fit_predict(scaled_embeddings)\n",
    "                \n",
    "                n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n",
    "                n_noise = np.sum(labels == -1)\n",
    "                \n",
    "                if n_clusters > 1:\n",
    "                    silhouette = silhouette_score(scaled_embeddings, labels)\n",
    "                else:\n",
    "                    silhouette = -1\n",
    "                \n",
    "                result = {\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'min_samples': min_samples,\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'n_noise': n_noise,\n",
    "                    'noise_ratio': n_noise / len(labels),\n",
    "                    'silhouette_score': silhouette\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                if silhouette > best_score:\n",
    "                    best_score = silhouette\n",
    "                    best_params = result\n",
    "                    \n",
    "                print(f\"    Clusters: {n_clusters}, Noise: {n_noise}, Silhouette: {silhouette:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"\\\\n✓ Best parameters found:\")\n",
    "    print(f\"  - min_cluster_size: {best_params['min_cluster_size']}\")\n",
    "    print(f\"  - min_samples: {best_params['min_samples']}\")\n",
    "    print(f\"  - Silhouette score: {best_params['silhouette_score']:.3f}\")\n",
    "    print(f\"  - Number of clusters: {best_params['n_clusters']}\")\n",
    "    \n",
    "    return best_params, results\n",
    "\n",
    "print(\"✓ HDBSCAN clustering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296850b",
   "metadata": {},
   "source": [
    "## Section 9: Visualize Cluster Assignments and Embedding Space\n",
    "\n",
    "Visualize the clustering results using PCA or t-SNE and plot cluster assignments over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2316e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embedding_space(embeddings, labels, method='PCA', sample_size=5000):\n",
    "    \"\"\"\n",
    "    Visualize embedding space using PCA or t-SNE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Embeddings to visualize\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels\n",
    "    method : str\n",
    "        Dimensionality reduction method ('PCA' or 'TSNE')\n",
    "    sample_size : int\n",
    "        Number of samples to visualize (for performance)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    # Sample data if too large\n",
    "    if len(embeddings) > sample_size:\n",
    "        indices = np.random.choice(len(embeddings), sample_size, replace=False)\n",
    "        embeddings_sample = embeddings[indices]\n",
    "        labels_sample = labels[indices]\n",
    "    else:\n",
    "        embeddings_sample = embeddings\n",
    "        labels_sample = labels\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    if method == 'PCA':\n",
    "        reducer = PCA(n_components=2)\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings_sample)\n",
    "        explained_var = reducer.explained_variance_ratio_\n",
    "        title = f\"PCA Visualization of Embedding Space\\\\n(Explained variance: {explained_var[0]:.2%} + {explained_var[1]:.2%} = {explained_var.sum():.2%})\"\n",
    "    elif method == 'TSNE':\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        reduced_embeddings = reducer.fit_transform(embeddings_sample)\n",
    "        title = \"t-SNE Visualization of Embedding Space\"\n",
    "    \n",
    "    # Create color map\n",
    "    unique_labels = np.unique(labels_sample)\n",
    "    colors = px.colors.qualitative.Set3 + px.colors.qualitative.Set1\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = labels_sample == label\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        if label == -1:\n",
    "            name = \"Noise\"\n",
    "            symbol = \"x\"\n",
    "        else:\n",
    "            name = f\"Cluster {label}\"\n",
    "            symbol = \"circle\"\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=reduced_embeddings[mask, 0],\n",
    "            y=reduced_embeddings[mask, 1],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=4,\n",
    "                color=color,\n",
    "                opacity=0.6,\n",
    "                symbol=symbol\n",
    "            ),\n",
    "            name=name,\n",
    "            text=[f\"Label: {label}<br>Point: {j}\" for j in np.where(mask)[0]],\n",
    "            hovertemplate='%{text}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=f\"{method} Component 1\",\n",
    "        yaxis_title=f\"{method} Component 2\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_cluster_timeline(metadata, labels, file_name=None):\n",
    "    \"\"\"\n",
    "    Plot cluster assignments over time.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metadata : list\n",
    "        List of metadata dictionaries\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels\n",
    "    file_name : str or None\n",
    "        Specific file to plot (if None, plot all files)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        Plotly figure object\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(metadata)\n",
    "    df['cluster'] = labels\n",
    "    \n",
    "    # Filter by file if specified\n",
    "    if file_name:\n",
    "        df = df[df['file_name'] == file_name]\n",
    "        title = f\"Cluster Timeline for {file_name}\"\n",
    "    else:\n",
    "        title = \"Cluster Timeline (All Files)\"\n",
    "    \n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Get unique clusters and colors\n",
    "    unique_clusters = sorted(df['cluster'].unique())\n",
    "    colors = px.colors.qualitative.Set3 + px.colors.qualitative.Set1\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        cluster_data = df[df['cluster'] == cluster]\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        if cluster == -1:\n",
    "            name = \"Noise\"\n",
    "            opacity = 0.5\n",
    "        else:\n",
    "            name = f\"Cluster {cluster}\"\n",
    "            opacity = 0.8\n",
    "        \n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=cluster_data['timestamp_start'],\n",
    "            y=cluster_data['cluster'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=color,\n",
    "                opacity=opacity,\n",
    "                symbol='square'\n",
    "            ),\n",
    "            name=name,\n",
    "            text=[f\"File: {row['file_name']}<br>Time: {row['timestamp_start']:.1f}s<br>Cluster: {row['cluster']}\" \n",
    "                  for _, row in cluster_data.iterrows()],\n",
    "            hovertemplate='%{text}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=\"Time (seconds)\",\n",
    "        yaxis_title=\"Cluster\",\n",
    "        width=1000,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Set y-axis to show discrete cluster values\n",
    "    fig.update_yaxis(\n",
    "        tickmode='array',\n",
    "        tickvals=unique_clusters,\n",
    "        ticktext=[f\"Cluster {c}\" if c != -1 else \"Noise\" for c in unique_clusters]\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_cluster_statistics(cluster_stats, labels):\n",
    "    \"\"\"\n",
    "    Plot cluster statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_stats : dict\n",
    "        Statistics about the clustering\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    cluster_sizes = [size for label, size in cluster_stats['cluster_sizes'].items() if label != -1]\n",
    "    cluster_labels_plot = [f\"Cluster {label}\" for label in cluster_stats['cluster_sizes'].keys() if label != -1]\n",
    "    \n",
    "    axes[0, 0].bar(cluster_labels_plot, cluster_sizes)\n",
    "    axes[0, 0].set_title('Cluster Size Distribution')\n",
    "    axes[0, 0].set_ylabel('Number of Points')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Cluster proportion pie chart\n",
    "    sizes = list(cluster_stats['cluster_sizes'].values())\n",
    "    labels_pie = [f\"Cluster {label}\" if label != -1 else \"Noise\" for label in cluster_stats['cluster_sizes'].keys()]\n",
    "    \n",
    "    axes[0, 1].pie(sizes, labels=labels_pie, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Cluster Proportion')\n",
    "    \n",
    "    # Histogram of cluster assignments\n",
    "    axes[1, 0].hist(labels, bins=np.arange(min(labels)-0.5, max(labels)+1.5, 1), alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Distribution of Cluster Assignments')\n",
    "    axes[1, 0].set_xlabel('Cluster Label')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Summary statistics\n",
    "    stats_text = f\"\"\"\n",
    "    Total Points: {cluster_stats['n_total']}\n",
    "    Number of Clusters: {cluster_stats['n_clusters']}\n",
    "    Noise Points: {cluster_stats['n_noise']} ({cluster_stats['noise_ratio']:.2%})\n",
    "    Silhouette Score: {cluster_stats['silhouette_score']:.3f}\n",
    "    \n",
    "    Largest Cluster: {max(cluster_sizes) if cluster_sizes else 0}\n",
    "    Smallest Cluster: {min(cluster_sizes) if cluster_sizes else 0}\n",
    "    Average Cluster Size: {np.mean(cluster_sizes):.1f} ± {np.std(cluster_sizes):.1f}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, stats_text, transform=axes[1, 1].transAxes, \n",
    "                    verticalalignment='top', fontsize=12, family='monospace')\n",
    "    axes[1, 1].set_title('Clustering Summary')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0c1da",
   "metadata": {},
   "source": [
    "## Section 10: Save Cluster Labels and Embeddings\n",
    "\n",
    "Save the predicted cluster labels and embeddings to disk for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clustering_results(embeddings, labels, metadata, cluster_stats, save_dir='results_3s'):\n",
    "    \"\"\"\n",
    "    Save all clustering results to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    embeddings : numpy.ndarray\n",
    "        Embeddings array\n",
    "    labels : numpy.ndarray\n",
    "        Cluster labels\n",
    "    metadata : list\n",
    "        Metadata for each embedding\n",
    "    cluster_stats : dict\n",
    "        Clustering statistics\n",
    "    save_dir : str\n",
    "        Directory to save results\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    file_paths : dict\n",
    "        Dictionary of saved file paths\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    base_name = f\"bilstm_transformer_hdbscan_3s_{timestamp}\"\n",
    "    \n",
    "    file_paths = {}\n",
    "    \n",
    "    # Save embeddings\n",
    "    emb_file = os.path.join(save_dir, f\"{base_name}_embeddings.npy\")\n",
    "    np.save(emb_file, embeddings)\n",
    "    file_paths['embeddings'] = emb_file\n",
    "    \n",
    "    # Save cluster labels\n",
    "    labels_file = os.path.join(save_dir, f\"{base_name}_labels.npy\")\n",
    "    np.save(labels_file, labels)\n",
    "    file_paths['labels'] = labels_file\n",
    "    \n",
    "    # Save metadata\n",
    "    meta_file = os.path.join(save_dir, f\"{base_name}_metadata.json\")\n",
    "    with open(meta_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    file_paths['metadata'] = meta_file\n",
    "    \n",
    "    # Save cluster statistics\n",
    "    stats_file = os.path.join(save_dir, f\"{base_name}_stats.json\")\n",
    "    with open(stats_file, 'w') as f:\n",
    "        # Convert numpy types to Python types for JSON serialization\n",
    "        json_stats = {}\n",
    "        for key, value in cluster_stats.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                json_stats[key] = value.tolist()\n",
    "            elif isinstance(value, dict):\n",
    "                json_stats[key] = {str(k): (v.tolist() if isinstance(v, np.ndarray) else v) \n",
    "                                  for k, v in value.items()}\n",
    "            else:\n",
    "                json_stats[key] = value\n",
    "        json.dump(json_stats, f, indent=2)\n",
    "    file_paths['stats'] = stats_file\n",
    "    \n",
    "    # Save combined CSV for easy analysis\n",
    "    csv_file = os.path.join(save_dir, f\"{base_name}_combined.csv\")\n",
    "    \n",
    "    # Create comprehensive DataFrame\n",
    "    df = pd.DataFrame(embeddings, columns=[f'emb_{i}' for i in range(embeddings.shape[1])])\n",
    "    df['cluster_label'] = labels\n",
    "    \n",
    "    # Add metadata columns\n",
    "    for key in metadata[0].keys():\n",
    "        df[key] = [m[key] for m in metadata]\n",
    "    \n",
    "    # Add additional derived columns\n",
    "    df['is_noise'] = (df['cluster_label'] == -1)\n",
    "    df['sequence_duration'] = df['timestamp_end'] - df['timestamp_start']\n",
    "    \n",
    "    df.to_csv(csv_file, index=False)\n",
    "    file_paths['csv'] = csv_file\n",
    "    \n",
    "    # Save summary report\n",
    "    report_file = os.path.join(save_dir, f\"{base_name}_report.txt\")\n",
    "    with open(report_file, 'w') as f:\n",
    "        f.write(\"BiLSTM + Transformer + HDBSCAN Clustering Results\\\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\\\n\")\n",
    "        f.write(f\"Total sequences: {len(embeddings)}\\\\n\")\n",
    "        f.write(f\"Embedding dimension: {embeddings.shape[1]}\\\\n\")\n",
    "        f.write(f\"Number of clusters: {cluster_stats['n_clusters']}\\\\n\")\n",
    "        f.write(f\"Noise points: {cluster_stats['n_noise']} ({cluster_stats['noise_ratio']:.2%})\\\\n\")\n",
    "        f.write(f\"Silhouette score: {cluster_stats['silhouette_score']:.3f}\\\\n\\\\n\")\n",
    "        \n",
    "        f.write(\"Cluster Sizes:\\\\n\")\n",
    "        for label, size in sorted(cluster_stats['cluster_sizes'].items()):\n",
    "            if label == -1:\n",
    "                f.write(f\"  Noise: {size} points\\\\n\")\n",
    "            else:\n",
    "                f.write(f\"  Cluster {label}: {size} points\\\\n\")\n",
    "        \n",
    "        f.write(\"\\\\nFiles processed:\\\\n\")\n",
    "        unique_files = list(set(m['file_name'] for m in metadata))\n",
    "        for file_name in sorted(unique_files):\n",
    "            count = sum(1 for m in metadata if m['file_name'] == file_name)\n",
    "            f.write(f\"  {file_name}: {count} sequences\\\\n\")\n",
    "    \n",
    "    file_paths['report'] = report_file\n",
    "    \n",
    "    print(f\"✓ Results saved to {save_dir}:\")\n",
    "    for key, path in file_paths.items():\n",
    "        print(f\"  - {key}: {os.path.basename(path)}\")\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "print(\"✓ Save functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd2f99",
   "metadata": {},
   "source": [
    "## Execution Workflow\n",
    "\n",
    "Now let's execute the complete pipeline step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c393a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Process EDF files and extract DMD features\n",
    "print(\"Step 1: Processing EDF files and extracting DMD features...\")\n",
    "feature_dict = process_all_files_dmd(edf_files, save_dir='features_3s')\n",
    "\n",
    "# Get input dimensions for model\n",
    "first_file = next(iter(feature_dict))\n",
    "input_dims = {\n",
    "    channel: data['features'].shape[1] \n",
    "    for channel, data in feature_dict[first_file].items()\n",
    "}\n",
    "\n",
    "print(f\"\\\\nInput dimensions: {input_dims}\")\n",
    "print(f\"Features extracted for {len(feature_dict)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create sequences for model input\n",
    "print(\"Step 2: Creating sequences for model input...\")\n",
    "sequences_dict = create_sequences_from_features(\n",
    "    feature_dict, \n",
    "    seq_length=20,  # 20 sequences of 3-second windows = 60 seconds total\n",
    "    overlap=0.5     # 50% overlap between sequences\n",
    ")\n",
    "\n",
    "print(f\"\\\\nSequences created for {len(sequences_dict)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de25066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train the BiLSTM + Transformer hybrid model\n",
    "print(\"Step 3: Training the BiLSTM + Transformer hybrid model...\")\n",
    "trained_model, training_history = train_hybrid_model(\n",
    "    sequences_dict=sequences_dict,\n",
    "    input_dims=input_dims,\n",
    "    epochs=30,\n",
    "    batch_size=16,\n",
    "    learning_rate=1e-4,\n",
    "    focus_channel='EEG Fpz-Cz'  # Focus on EEG channel for reconstruction\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26589e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Extract embeddings from trained model\n",
    "print(\"Step 4: Extracting embeddings from trained model...\")\n",
    "embeddings, metadata = extract_embeddings_from_model(\n",
    "    model=trained_model,\n",
    "    sequences_dict=sequences_dict,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"\\\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Metadata entries: {len(metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0953241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Optimize HDBSCAN parameters and perform clustering\n",
    "print(\"Step 5: Optimizing HDBSCAN parameters and performing clustering...\")\n",
    "\n",
    "# Optimize parameters (optional - can skip for faster execution)\n",
    "best_params, optimization_results = optimize_hdbscan_parameters(\n",
    "    embeddings,\n",
    "    min_cluster_sizes=[30, 50, 75, 100],\n",
    "    min_samples_list=[5, 10, 15]\n",
    ")\n",
    "\n",
    "# Perform clustering with optimized parameters\n",
    "cluster_labels, clusterer, cluster_stats = cluster_embeddings_hdbscan(\n",
    "    embeddings,\n",
    "    min_cluster_size=best_params['min_cluster_size'],\n",
    "    min_samples=best_params['min_samples']\n",
    ")\n",
    "\n",
    "print(f\"\\\\nClustering completed with {cluster_stats['n_clusters']} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Visualize results\n",
    "print(\"Step 6: Visualizing clustering results...\")\n",
    "\n",
    "# Plot cluster statistics\n",
    "plot_cluster_statistics(cluster_stats, cluster_labels)\n",
    "\n",
    "# Visualize embedding space with PCA\n",
    "fig_pca = visualize_embedding_space(embeddings, cluster_labels, method='PCA')\n",
    "fig_pca.show()\n",
    "\n",
    "# Visualize embedding space with t-SNE (optional - takes longer)\n",
    "# fig_tsne = visualize_embedding_space(embeddings, cluster_labels, method='TSNE')\n",
    "# fig_tsne.show()\n",
    "\n",
    "# Plot cluster timeline\n",
    "fig_timeline = plot_cluster_timeline(metadata, cluster_labels)\n",
    "fig_timeline.show()\n",
    "\n",
    "# Plot timeline for specific file\n",
    "first_file = metadata[0]['file_name']\n",
    "fig_timeline_single = plot_cluster_timeline(metadata, cluster_labels, file_name=first_file)\n",
    "fig_timeline_single.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d78cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Save all results\n",
    "print(\"Step 7: Saving all results...\")\n",
    "\n",
    "# Save clustering results\n",
    "file_paths = save_clustering_results(\n",
    "    embeddings=embeddings,\n",
    "    labels=cluster_labels,\n",
    "    metadata=metadata,\n",
    "    cluster_stats=cluster_stats,\n",
    "    save_dir='results_3s'\n",
    ")\n",
    "\n",
    "# Save embeddings separately\n",
    "save_embeddings(embeddings, metadata, save_path='embeddings_3s')\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"🎉 PIPELINE EXECUTION COMPLETED SUCCESSFULLY! 🎉\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\\\nSummary:\")\n",
    "print(f\"- Processed {len(edf_files)} EDF files\")\n",
    "print(f\"- Extracted {len(embeddings)} embeddings from 3-second windows\")\n",
    "print(f\"- Discovered {cluster_stats['n_clusters']} distinct clusters\")\n",
    "print(f\"- Silhouette score: {cluster_stats['silhouette_score']:.3f}\")\n",
    "print(f\"- Results saved to: results_3s/\")\n",
    "print(\"\\\\nNext steps:\")\n",
    "print(\"- Analyze cluster characteristics\")\n",
    "print(\"- Correlate with sleep stage annotations (if available)\")\n",
    "print(\"- Perform statistical analysis of cluster transitions\")\n",
    "print(\"- Generate detailed reports\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
